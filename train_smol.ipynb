{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb5e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli download Lava8888/pnp_small --repo-type=dataset --local-dir ./pnp_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af14c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.name \"<LudvigEriksonBrangstrup>\"\n",
    "!git config --global user.email \"Ludvig.Brangstrup@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b15f7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers==4.50.3\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers==4.50.3) (2.25.1)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 KB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.8/485.8 KB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.17\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.50.3) (25.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2025.7.34-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (789 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m789.8/789.8 KB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers==4.50.3) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.50.3) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.3) (4.14.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3\n",
      "  Downloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m143.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.6/199.6 KB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, numpy, hf-xet, fsspec, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed fsspec-2025.7.0 hf-xet-1.1.7 huggingface-hub-0.34.4 numpy-2.2.6 regex-2025.7.34 safetensors-0.6.2 tokenizers-0.21.4 tqdm-4.67.1 transformers-4.50.3\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting num2words\n",
      "  Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docopt>=0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=a51fa774d0f6d9fb59483901f8e12949284595d7b0676f8b6568ea5ecf519f38\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, num2words\n",
      "Successfully installed docopt-0.6.2 num2words-0.5.14\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.7/374.7 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<3.0.0,>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (2.2.6)\n",
      "Collecting torch>=2.0.0\n",
      "  Downloading torch-2.8.0-cp310-cp310-manylinux_2_28_x86_64.whl (888.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.0/888.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (0.34.4)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.25.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.7)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.10.2.21\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.9.90\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.3.3.83\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.4.0\n",
      "  Downloading triton-3.4.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.8.90\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 KB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.7.1\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.8.93\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=2.0.0->accelerate) (3.0.3)\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufile-cu12==1.13.1.3\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.8.90\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.8.4.1\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.8.90\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 KB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.8.93\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.27.3\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.7.3.90\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.4.0->torch>=2.0.0->accelerate) (59.6.0)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, accelerate\n",
      "Successfully installed accelerate-1.10.0 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 sympy-1.14.0 torch-2.8.0 triton-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.50.3\n",
    "!pip install num2words\n",
    "!pip install accelerate\n",
    "!pip install safetensors>=0.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bddd978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting lerobot\n",
      "  Cloning https://github.com/huggingface/lerobot.git (to revision 10b7b3532543b4adfb65760f02a49b4c537afde7) to /tmp/pip-install-odwyopl3/lerobot_b446afe307a84ac7a630d7dec94a42f9\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/lerobot.git /tmp/pip-install-odwyopl3/lerobot_b446afe307a84ac7a630d7dec94a42f9\n",
      "  Running command git rev-parse -q --verify 'sha^10b7b3532543b4adfb65760f02a49b4c537afde7'\n",
      "  Running command git fetch -q https://github.com/huggingface/lerobot.git 10b7b3532543b4adfb65760f02a49b4c537afde7\n",
      "  Running command git checkout -q 10b7b3532543b4adfb65760f02a49b4c537afde7\n",
      "  Resolved https://github.com/huggingface/lerobot.git to commit 10b7b3532543b4adfb65760f02a49b4c537afde7\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyautogui\n",
      "  Downloading PyAutoGUI-0.9.54.tar.gz (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.10.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pytweening>=1.0.4\n",
      "  Downloading pytweening-1.2.0.tar.gz (171 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.2/171.2 KB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pygetwindow>=0.0.5\n",
      "  Downloading PyGetWindow-0.0.9.tar.gz (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting mouseinfo\n",
      "  Downloading MouseInfo-0.1.3.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting python3-Xlib\n",
      "  Downloading python3-xlib-0.15.tar.gz (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 KB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyscreeze>=0.1.21\n",
      "  Downloading pyscreeze-1.0.1.tar.gz (27 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pymsgbox\n",
      "  Downloading PyMsgBox-1.0.9.tar.gz (18 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (25.0)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow>=8\n",
      "  Downloading pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m128.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.59.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m136.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 KB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.2.6)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting gymnasium==0.29.1\n",
      "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 KB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opencv-python-headless>=4.9.0\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=2.4.0\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting jsonlines>=4.0.0\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Collecting h5py>=3.10.0\n",
      "  Downloading h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.2.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from lerobot->-r requirements.txt (line 4)) (2.8.0)\n",
      "Collecting torchvision>=0.21.0\n",
      "  Downloading torchvision-0.23.0-cp310-cp310-manylinux_2_28_x86_64.whl (8.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting flask>=3.0.3\n",
      "  Downloading flask-3.1.1-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 KB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyzmq>=26.2.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from lerobot->-r requirements.txt (line 4)) (27.0.1)\n",
      "Collecting einops>=0.8.0\n",
      "  Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 KB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rerun-sdk>=0.21.0\n",
      "  Downloading rerun_sdk-0.24.1-cp39-abi3-manylinux_2_28_x86_64.whl (90.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pymunk<7.0.0,>=6.6.0\n",
      "  Downloading pymunk-6.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gdown>=5.1.0\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Collecting diffusers>=0.27.2\n",
      "  Downloading diffusers-0.34.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchcodec>=0.2.1\n",
      "  Downloading torchcodec-0.6.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting omegaconf>=2.3.0\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting av>=14.2.0\n",
      "  Downloading av-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.2/39.2 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting imageio[ffmpeg]>=2.34.0\n",
      "  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.8/315.8 KB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting deepdiff>=7.0.1\n",
      "  Downloading deepdiff-8.6.0-py3-none-any.whl (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 KB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub[cli,hf-transfer]>=0.27.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from lerobot->-r requirements.txt (line 4)) (0.34.4)\n",
      "Collecting cmake>=3.29.0.1\n",
      "  Downloading cmake-4.0.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting zarr>=2.17.0\n",
      "  Downloading zarr-2.18.3-py3-none-any.whl (210 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 KB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wandb>=0.16.3\n",
      "  Downloading wandb-0.21.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyserial>=3.5 in /usr/lib/python3/dist-packages (from lerobot->-r requirements.txt (line 4)) (3.5)\n",
      "Collecting draccus==0.10.0\n",
      "  Downloading draccus-0.10.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets>=2.19.0\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 KB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numba>=0.59.0\n",
      "  Downloading numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m144.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pynput>=1.7.7\n",
      "  Downloading pynput-1.8.1-py2.py3-none-any.whl (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.7/91.7 KB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml-include~=1.4\n",
      "  Downloading pyyaml_include-1.4.1-py3-none-any.whl (19 kB)\n",
      "Collecting toml~=0.10\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting typing-inspect~=0.9.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting mergedeep~=1.3\n",
      "  Downloading mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\n",
      "Collecting pyyaml~=6.0\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 KB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting farama-notifications>=0.0.1\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from gymnasium==0.29.1->lerobot->-r requirements.txt (line 4)) (4.14.1)\n",
      "Collecting cloudpickle>=1.2.0\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting fsspec[http]<=2025.3.0,>=2023.1.0\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 KB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets>=2.19.0->lerobot->-r requirements.txt (line 4)) (3.6.0)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m138.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets>=2.19.0->lerobot->-r requirements.txt (line 4)) (4.67.1)\n",
      "Collecting requests>=2.32.2\n",
      "  Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 KB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orderly-set<6,>=5.4.1\n",
      "  Downloading orderly_set-5.5.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from diffusers>=0.27.2->lerobot->-r requirements.txt (line 4)) (2025.7.34)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from diffusers>=0.27.2->lerobot->-r requirements.txt (line 4)) (0.6.2)\n",
      "Requirement already satisfied: importlib_metadata in /usr/lib/python3/dist-packages (from diffusers>=0.27.2->lerobot->-r requirements.txt (line 4)) (4.6.4)\n",
      "Collecting click>=8.1.3\n",
      "  Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 KB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting blinker>=1.9.0\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Collecting itsdangerous>=2.2.0\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting markupsafe>=2.1.1\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Collecting jinja2>=3.1.2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 KB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=3.1.0\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 KB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.3/187.3 KB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests[socks] in /usr/lib/python3/dist-packages (from gdown>=5.1.0->lerobot->-r requirements.txt (line 4)) (2.25.1)\n",
      "\u001b[33mWARNING: huggingface-hub 0.34.4 does not provide the extra 'hf-transfer'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub[cli,hf-transfer]>=0.27.1->lerobot->-r requirements.txt (line 4)) (1.1.7)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub[cli,hf-transfer]>=0.27.1->lerobot->-r requirements.txt (line 4)) (2025.7.0)\n",
      "Collecting InquirerPy==0.3.4\n",
      "  Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 KB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pfzy<0.4.0,>=0.3.1\n",
      "  Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]>=0.27.1->lerobot->-r requirements.txt (line 4)) (3.0.51)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.local/lib/python3.10/site-packages (from imageio[ffmpeg]>=2.34.0->lerobot->-r requirements.txt (line 4)) (7.0.0)\n",
      "Collecting imageio-ffmpeg\n",
      "  Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl (29.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/lib/python3/dist-packages (from jsonlines>=4.0.0->lerobot->-r requirements.txt (line 4)) (21.2.0)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0\n",
      "  Downloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting antlr4-python3-runtime==4.9.*\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 KB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyrect\n",
      "  Downloading PyRect-0.2.0.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cffi>=1.17.1\n",
      "  Downloading cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m446.2/446.2 KB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-xlib>=0.17\n",
      "  Downloading python_xlib-0.33-py2.py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.2/182.2 KB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from pynput>=1.7.7->lerobot->-r requirements.txt (line 4)) (1.16.0)\n",
      "Collecting evdev>=1.3\n",
      "  Downloading evdev-1.9.2.tar.gz (33 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package evdev produced metadata for project name unknown. Fix your #egg=evdev fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/63/fe/a17c106a1f4061ce83f04d14bcedcfb2c38c7793ea56bfb906a6fadae8cb/evdev-1.9.2.tar.gz#sha256=5d3278892ce1f92a74d6bf888cc8525d9f68af85dbe336c95d1c87fb8f423069 (from https://pypi.org/simple/evdev/) (requires-python:>=3.8)\u001b[0m: \u001b[33mRequested unknown from https://files.pythonhosted.org/packages/63/fe/a17c106a1f4061ce83f04d14bcedcfb2c38c7793ea56bfb906a6fadae8cb/evdev-1.9.2.tar.gz#sha256=5d3278892ce1f92a74d6bf888cc8525d9f68af85dbe336c95d1c87fb8f423069 (from pynput>=1.7.7->lerobot->-r requirements.txt (line 4)) has inconsistent name: filename has 'evdev', but metadata has 'unknown'\u001b[0m\n",
      "  Downloading evdev-1.9.1.tar.gz (33 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package evdev produced metadata for project name unknown. Fix your #egg=evdev fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/d1/99/4d24bb6db12fc170a5f209f4c9108054a2c84d289d1e7f743e979b202023/evdev-1.9.1.tar.gz#sha256=dc640a064cb1c9fe1f8b970dc2039945a2a275d7b7ee62284bf427238abe45ee (from https://pypi.org/simple/evdev/) (requires-python:>=3.8)\u001b[0m: \u001b[33mRequested unknown from https://files.pythonhosted.org/packages/d1/99/4d24bb6db12fc170a5f209f4c9108054a2c84d289d1e7f743e979b202023/evdev-1.9.1.tar.gz#sha256=dc640a064cb1c9fe1f8b970dc2039945a2a275d7b7ee62284bf427238abe45ee (from pynput>=1.7.7->lerobot->-r requirements.txt (line 4)) has inconsistent name: filename has 'evdev', but metadata has 'unknown'\u001b[0m\n",
      "  Downloading evdev-1.9.0.tar.gz (32 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package evdev produced metadata for project name unknown. Fix your #egg=evdev fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/8c/bf/0fe4d372ee62fe5aec06aae2e97c0603d131b8e3cbaffa2329e0bd763a98/evdev-1.9.0.tar.gz#sha256=38218bc95507cad756bc621a5fe7a77a41eb8d9c1213855de8e4ba0aba28e79f (from https://pypi.org/simple/evdev/) (requires-python:>=3.8)\u001b[0m: \u001b[33mRequested unknown from https://files.pythonhosted.org/packages/8c/bf/0fe4d372ee62fe5aec06aae2e97c0603d131b8e3cbaffa2329e0bd763a98/evdev-1.9.0.tar.gz#sha256=38218bc95507cad756bc621a5fe7a77a41eb8d9c1213855de8e4ba0aba28e79f (from pynput>=1.7.7->lerobot->-r requirements.txt (line 4)) has inconsistent name: filename has 'evdev', but metadata has 'unknown'\u001b[0m\n",
      "  Downloading evdev-1.8.0.tar.gz (32 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package evdev produced metadata for project name unknown. Fix your #egg=evdev fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/4c/eb/b8900082a13a47bb9f69f415174d03cbf12ce95d07345722e1f4ef0e2093/evdev-1.8.0.tar.gz#sha256=45598eee1ae3876a3122ca1dc0ec8049c01931672d12478b5c610afc24e47d75 (from https://pypi.org/simple/evdev/) (requires-python:>=3.8)\u001b[0m: \u001b[33mRequested unknown from https://files.pythonhosted.org/packages/4c/eb/b8900082a13a47bb9f69f415174d03cbf12ce95d07345722e1f4ef0e2093/evdev-1.8.0.tar.gz#sha256=45598eee1ae3876a3122ca1dc0ec8049c01931672d12478b5c610afc24e47d75 (from pynput>=1.7.7->lerobot->-r requirements.txt (line 4)) has inconsistent name: filename has 'evdev', but metadata has 'unknown'\u001b[0m\n",
      "  Downloading evdev-1.7.1.tar.gz (30 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package evdev produced metadata for project name unknown. Fix your #egg=evdev fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/12/bb/f622a8a5e64d46ca83020a761877c0ead19140903c9aaf1431f3c531fdf6/evdev-1.7.1.tar.gz#sha256=0c72c370bda29d857e188d931019c32651a9c1ea977c08c8d939b1ced1637fde (from https://pypi.org/simple/evdev/) (requires-python:>=3.6)\u001b[0m: \u001b[33mRequested unknown from https://files.pythonhosted.org/packages/12/bb/f622a8a5e64d46ca83020a761877c0ead19140903c9aaf1431f3c531fdf6/evdev-1.7.1.tar.gz#sha256=0c72c370bda29d857e188d931019c32651a9c1ea977c08c8d939b1ced1637fde (from pynput>=1.7.7->lerobot->-r requirements.txt (line 4)) has inconsistent name: filename has 'evdev', but metadata has 'unknown'\u001b[0m\n",
      "  Downloading evdev-1.7.0.tar.gz (30 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package evdev produced metadata for project name unknown. Fix your #egg=evdev fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/f3/0c/c87b141250a650ee53d4e9957e905b17c144cda6dc4187cbe89a641a1b34/evdev-1.7.0.tar.gz#sha256=95bd2a1e0c6ce2cd7a2ecc6e6cd9736ff794b3ad5cb54d81d8cbc2e414d0b870 (from https://pypi.org/simple/evdev/) (requires-python:>=3.6)\u001b[0m: \u001b[33mRequested unknown from https://files.pythonhosted.org/packages/f3/0c/c87b141250a650ee53d4e9957e905b17c144cda6dc4187cbe89a641a1b34/evdev-1.7.0.tar.gz#sha256=95bd2a1e0c6ce2cd7a2ecc6e6cd9736ff794b3ad5cb54d81d8cbc2e414d0b870 (from pynput>=1.7.7->lerobot->-r requirements.txt (line 4)) has inconsistent name: filename has 'evdev', but metadata has 'unknown'\u001b[0m\n",
      "  Downloading evdev-1.6.1.tar.gz (26 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting attrs>=19.2.0\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 KB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (0.7.1)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (2.27.3)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (1.13.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (10.3.9.90)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (11.7.3.90)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.4.0->torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (59.6.0)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.2/208.2 KB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<3\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 KB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs in /usr/lib/python3/dist-packages (from wandb>=0.16.3->lerobot->-r requirements.txt (line 4)) (2.5.1)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.1/321.1 KB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentry-sdk>=2.0.0\n",
      "  Downloading sentry_sdk-2.34.1-py2.py3-none-any.whl (357 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.7/357.7 KB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numcodecs>=0.10.0\n",
      "  Downloading numcodecs-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting asciitree\n",
      "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fasteners\n",
      "  Downloading fasteners-0.20-py3-none-any.whl (18 kB)\n",
      "Collecting pyperclip\n",
      "  Downloading pyperclip-1.9.0.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pycparser\n",
      "  Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 KB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.12.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 KB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets>=2.19.0->lerobot->-r requirements.txt (line 4)) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets>=2.19.0->lerobot->-r requirements.txt (line 4)) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets>=2.19.0->lerobot->-r requirements.txt (line 4)) (3.3)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 KB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.2.1->lerobot->-r requirements.txt (line 4)) (1.3.0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets>=2.19.0->lerobot->-r requirements.txt (line 4)) (2022.1)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 KB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.4.0\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.3/198.3 KB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.1/326.1 KB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.6.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.6/241.6 KB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.9/222.9 KB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: wcwidth in /home/ubuntu/.local/lib/python3.10/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]>=0.27.1->lerobot->-r requirements.txt (line 4)) (0.2.13)\n",
      "Building wheels for collected packages: pyautogui, lerobot, antlr4-python3-runtime, pygetwindow, pyscreeze, pytweening, mouseinfo, pymsgbox, python3-Xlib, evdev, asciitree, pyperclip, pyrect\n",
      "  Building wheel for pyautogui (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyautogui: filename=PyAutoGUI-0.9.54-py3-none-any.whl size=37592 sha256=5f3bedccab066d30d7f53f72029c7c3936a0dec39c13b0b07551c1f5a53fc44c\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/23/a7/1c/5a51aaff3bbe110be4ddf766d429cc9d2fae7a72fc1b843e56\n",
      "  Building wheel for lerobot (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lerobot: filename=lerobot-0.1.0-py3-none-any.whl size=455543 sha256=337d9cba502d9b8347646ec434c591ba3498803878e454542485dec6a52777d5\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/8e/f8/fe/9659b3297a12eed07099d3bcfb27550ed2c26e13f2096d6bfb\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=a862b190a9d17f1582b8d8682039507c6b609a1e7b115ebf6a16f5e81d553c9b\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "  Building wheel for pygetwindow (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pygetwindow: filename=PyGetWindow-0.0.9-py3-none-any.whl size=11081 sha256=f0794b408114f96a16b7c13f321f6fb0b70ef414e89a65c45018929f51e3dfa4\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/02/f6/64/c5d427819f80553df2398bfecc351e94e00371c1dcb6edb24e\n",
      "  Building wheel for pyscreeze (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyscreeze: filename=PyScreeze-1.0.1-py3-none-any.whl size=14381 sha256=b1419ae314a07e65c986e9ade20f582ef06546709cc3808f67faf56653eac905\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/61/bb/e6/638ce7843ec76f69155decf3a8f2c0ef0e6afc53fc9e09c7e8\n",
      "  Building wheel for pytweening (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytweening: filename=pytweening-1.2.0-py3-none-any.whl size=8024 sha256=05fd0a8f1d63a4f24f35a7691e2d014f9cf4462e108c5286b63ad4a6546ae16f\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/22/d7/02/b3e395d93b6dd41a7da54b2fa738ec03e7fb7451f7f22f8213\n",
      "  Building wheel for mouseinfo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mouseinfo: filename=MouseInfo-0.1.3-py3-none-any.whl size=10906 sha256=ade53327be0abf5b484f3ffc83386df0c590c1301cd5e0ea289fb1dab425f567\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/6e/d5/27/2f1be84b3e6ccee99c82f50e3fe7fe6360dd30417109b49a72\n",
      "  Building wheel for pymsgbox (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pymsgbox: filename=PyMsgBox-1.0.9-py3-none-any.whl size=7420 sha256=3f83efbbcbb4453932b2ed1c08ab73116c0615d7012572b95ea9db7b16cbe433\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/b9/6a/ba/be2d7d78166ec8018c21d07241dffa54446c09652a267759ae\n",
      "  Building wheel for python3-Xlib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python3-Xlib: filename=python3_xlib-0.15-py3-none-any.whl size=109517 sha256=29a1f16b7a6933dd175ae4524923aa51deb02a32152b7152af24a2891adc0e6f\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/9c/8e/c5/8a93a3415a4a2065f31750a3244db61482b9e508f04ee56e82\n",
      "  Building wheel for evdev (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for evdev: filename=evdev-1.6.1-cp310-cp310-linux_x86_64.whl size=83779 sha256=08bab5c990ff1a3327cafa2be2ce5f3d2d7e0d0e4e607cecc089f9e9b7cec52c\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/bb/6e/51/0c0f5677bd58c532c42e13e7d93803fbf8d5d3bf5bfc5b0220\n",
      "  Building wheel for asciitree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5050 sha256=d2c2ee0d3939bb8ec0210b5ccc7ef524477b5da2556dd1ab29203798bfa1ae0a\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/7f/4e/be/1171b40f43b918087657ec57cf3b81fa1a2e027d8755baa184\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyperclip: filename=pyperclip-1.9.0-py3-none-any.whl size=11014 sha256=5a6cea59f4ff9d7ca5aa98d5704ba23a37aad7ab226c98c01979aa14d56131a5\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/cc/ae/36/ee17d1de094fcb61e24106cb329b5103861e819f94bef5e10a\n",
      "  Building wheel for pyrect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyrect: filename=PyRect-0.2.0-py2.py3-none-any.whl size=11196 sha256=9bb4e185e4ce2d3dabc8eb2983771a13d922cbd0306056a485d00fecc76e8b0b\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/d5/4c/bd/42e4e23641afcd185d4e932784da37e6e04505da0cf3f7b832\n",
      "Successfully built pyautogui lerobot antlr4-python3-runtime pygetwindow pyscreeze pytweening mouseinfo pymsgbox python3-Xlib evdev asciitree pyperclip pyrect\n",
      "Installing collected packages: pytweening, python3-Xlib, pyrect, pyperclip, pymsgbox, farama-notifications, evdev, asciitree, antlr4-python3-runtime, xxhash, urllib3, tzdata, typing-inspection, torchcodec, toml, termcolor, soupsieve, smmap, scipy, pyyaml, python-xlib, PySocks, pygetwindow, pydantic-core, pycparser, pyarrow, protobuf, propcache, pillow, pfzy, orderly-set, opencv-python-headless, numcodecs, mypy-extensions, multidict, mouseinfo, mergedeep, markupsafe, llvmlite, kiwisolver, itsdangerous, imageio-ffmpeg, h5py, fsspec, frozenlist, fonttools, fasteners, einops, dill, cycler, contourpy, cmake, cloudpickle, click, charset_normalizer, blinker, av, attrs, async-timeout, annotated-types, aiohappyeyeballs, zarr, yarl, werkzeug, typing-inspect, sentry-sdk, rerun-sdk, requests, pyyaml-include, pyscreeze, pynput, pydantic, pandas, omegaconf, numba, multiprocess, matplotlib, jsonlines, jinja2, InquirerPy, imageio, gymnasium, gitdb, deepdiff, cffi, beautifulsoup4, aiosignal, pymunk, pyautogui, gitpython, flask, draccus, aiohttp, wandb, torchvision, gdown, diffusers, datasets, lerobot\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.7.0\n",
      "    Uninstalling fsspec-2025.7.0:\n",
      "      Successfully uninstalled fsspec-2025.7.0\n",
      "Successfully installed InquirerPy-0.3.4 PySocks-1.7.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 asciitree-0.3.3 async-timeout-5.0.1 attrs-25.3.0 av-15.0.0 beautifulsoup4-4.13.4 blinker-1.9.0 cffi-1.17.1 charset_normalizer-3.4.3 click-8.2.1 cloudpickle-3.1.1 cmake-4.0.3 contourpy-1.3.2 cycler-0.12.1 datasets-4.0.0 deepdiff-8.6.0 diffusers-0.34.0 dill-0.3.8 draccus-0.10.0 einops-0.8.1 evdev-1.6.1 farama-notifications-0.0.4 fasteners-0.20 flask-3.1.1 fonttools-4.59.0 frozenlist-1.7.0 fsspec-2025.3.0 gdown-5.2.0 gitdb-4.0.12 gitpython-3.1.45 gymnasium-0.29.1 h5py-3.14.0 imageio-2.37.0 imageio-ffmpeg-0.6.0 itsdangerous-2.2.0 jinja2-3.1.6 jsonlines-4.0.0 kiwisolver-1.4.9 lerobot-0.1.0 llvmlite-0.44.0 markupsafe-3.0.2 matplotlib-3.10.5 mergedeep-1.3.4 mouseinfo-0.1.3 multidict-6.6.4 multiprocess-0.70.16 mypy-extensions-1.1.0 numba-0.61.2 numcodecs-0.13.1 omegaconf-2.3.0 opencv-python-headless-4.12.0.88 orderly-set-5.5.0 pandas-2.3.1 pfzy-0.3.4 pillow-11.3.0 propcache-0.3.2 protobuf-6.31.1 pyarrow-21.0.0 pyautogui-0.9.54 pycparser-2.22 pydantic-2.11.7 pydantic-core-2.33.2 pygetwindow-0.0.9 pymsgbox-1.0.9 pymunk-6.11.1 pynput-1.8.1 pyperclip-1.9.0 pyrect-0.2.0 pyscreeze-1.0.1 python-xlib-0.33 python3-Xlib-0.15 pytweening-1.2.0 pyyaml-6.0.2 pyyaml-include-1.4.1 requests-2.32.4 rerun-sdk-0.24.1 scipy-1.15.3 sentry-sdk-2.34.1 smmap-5.0.2 soupsieve-2.7 termcolor-3.1.0 toml-0.10.2 torchcodec-0.6.0 torchvision-0.23.0 typing-inspect-0.9.0 typing-inspection-0.4.1 tzdata-2025.2 urllib3-2.5.0 wandb-0.21.1 werkzeug-3.1.3 xxhash-3.5.0 yarl-1.20.1 zarr-2.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb3d0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface_hub in /home/ubuntu/.local/lib/python3.10/site-packages (0.34.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface_hub) (3.6.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub) (1.1.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface_hub) (2.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4acf8bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 61, in main\n",
      "    service.run()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/commands/user.py\", line 113, in run\n",
      "    login(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 31, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 130, in login\n",
      "    interpreter_login(new_session=new_session)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 31, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 287, in interpreter_login\n",
      "    token = getpass(\"Enter your token (input will not be visible): \")\n",
      "  File \"/usr/lib/python3.10/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "  File \"/usr/lib/python3.10/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b381612e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
      "Fetching 60 files:   0%|                                 | 0/60 [00:00<?, ?it/s]Downloading 'data/chunk-000/episode_000004.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/jFtolRe5Vvw8HkW1jrJcK1IP93I=.c306330dd4605ef8c4dbc1603be2e854789d99756156ca7c345d6d81aa897bc7.incomplete'\n",
      "Downloading 'data/chunk-000/episode_000003.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/I4KmEhnhp7DGsGV1cF9fTGwH1Z8=.be83b8b15cfbfaac019b2b1a7c326e46ee1cdccba2bccfb94dbe2229653928a9.incomplete'\n",
      "Downloading 'data/chunk-000/episode_000006.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/oo-hXxuwhJ7RGGbeILD7o43Hu6o=.723fe25624e6bc1cce8936eb43f2135b44719c80600b7a6c9e3c4e7def55ce1d.incomplete'\n",
      "Downloading 'data/chunk-000/episode_000000.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/cNsBuwyxGLgaOECPx2lW57L780o=.d31e0bdd4312d099e9dc657a23fb01214d6541f4ba2e81f212727e8f30e42c0e.incomplete'\n",
      "Downloading '.gitattributes' to 'PnPCounterToSink/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.1ef325f1b111266a6b26e0196871bd78baa8c2f3.incomplete'\n",
      "\n",
      ".gitattributes: 2.46kB [00:00, 11.9MB/s]\n",
      "Download complete. Moving file to PnPCounterToSink/.gitattributes\n",
      "Downloading 'data/chunk-000/episode_000005.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/-SK4qvRTwf8GI3UnEqFxvRjn4fA=.6f0384cedbfc00b8e7a5278300daf1399582f317f2bdb3e5658a2e674fd9244b.incomplete'\n",
      "\n",
      "episode_000003.parquet:   0%|                       | 0.00/36.6M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "episode_000006.parquet:   0%|                       | 0.00/62.4M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000000.parquet:   0%|                       | 0.00/63.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000004.parquet:   0%|                       | 0.00/43.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000005.parquet:   0%|                       | 0.00/37.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000002.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/OSyyJRwGKrO_YFS2cpjykP4P8nE=.45cf2a369c3c485bb9a9dc2f6ecf5d94ece96dee33efeb911705ffb6b927d181.incomplete'\n",
      "Downloading 'data/chunk-000/episode_000001.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/VextNUIRmXdKTDFi2d6W2hth3P4=.01699bb9f8812614638ff26e7868bc3b8c71e339d6b7c6672afb09a418d6d471.incomplete'\n",
      "Downloading 'data/chunk-000/episode_000007.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/1d533h4ypW6i_-P9ejcguxrCaPM=.0c58f3d314e9a38c0873e16ced6489727dd583f66fdfe4a0da50b7f93c915fbc.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000001.parquet:   0%|                       | 0.00/54.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000002.parquet:   0%|                       | 0.00/83.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000007.parquet:   0%|                       | 0.00/41.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000006.parquet:  17%|██▎           | 10.5M/62.4M [00:00<00:00, 85.8MB/s]\u001b[A\u001b[A\n",
      "episode_000003.parquet:  29%|████          | 10.5M/36.6M [00:00<00:00, 62.3MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000004.parquet:  24%|███▍          | 10.5M/43.5M [00:00<00:00, 50.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000000.parquet:  33%|████▋         | 21.0M/63.1M [00:00<00:00, 85.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000006.parquet:  34%|████▋         | 21.0M/62.4M [00:00<00:00, 72.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000005.parquet:  28%|███▉          | 10.5M/37.7M [00:00<00:00, 38.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000002.parquet:  13%|█▊            | 10.5M/83.5M [00:00<00:01, 45.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000007.parquet:  25%|███▌          | 10.5M/41.9M [00:00<00:00, 37.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000004.parquet:  48%|██████▊       | 21.0M/43.5M [00:00<00:00, 51.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000001.parquet:  19%|██▋           | 10.5M/54.9M [00:00<00:01, 30.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000003.parquet:  57%|████████      | 21.0M/36.6M [00:00<00:00, 43.3MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000005.parquet:  56%|███████▊      | 21.0M/37.7M [00:00<00:00, 46.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000000.parquet:  50%|██████▉       | 31.5M/63.1M [00:00<00:00, 58.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000006.parquet:  50%|███████       | 31.5M/62.4M [00:00<00:00, 56.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000002.parquet:  25%|███▌          | 21.0M/83.5M [00:00<00:01, 43.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000001.parquet:  38%|█████▎        | 21.0M/54.9M [00:00<00:00, 40.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000004.parquet:  72%|██████████▏   | 31.5M/43.5M [00:00<00:00, 49.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000005.parquet:  83%|███████████▋  | 31.5M/37.7M [00:00<00:00, 52.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000003.parquet:  86%|████████████  | 31.5M/36.6M [00:00<00:00, 47.2MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000007.parquet:  50%|███████       | 21.0M/41.9M [00:00<00:00, 37.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000000.parquet:  66%|█████████▎    | 41.9M/63.1M [00:00<00:00, 57.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000003.parquet: 100%|██████████████| 36.6M/36.6M [00:00<00:00, 49.6MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000003.parquet\n",
      "episode_000005.parquet: 100%|██████████████| 37.7M/37.7M [00:00<00:00, 51.3MB/s]\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000005.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000002.parquet:  38%|█████▎        | 31.5M/83.5M [00:00<00:01, 47.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000008.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/FNURp8HeA8-i1yewJ18DZKy6hBE=.547c7d2ef9509f5f6ae07df755079b9d6dc32c6dafb48360e4315f1c841ed144.incomplete'\n",
      "\n",
      "episode_000008.parquet:   0%|                       | 0.00/54.1M [00:00<?, ?B/s]\u001b[ADownloading 'data/chunk-000/episode_000009.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/L1U0W438Fu_YRcBknvo4psb4yGo=.4c3477f0daabf8b957e7362e17d57aac3e5a0305170dbc68acaf9d6a9bef07ac.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000009.parquet:   0%|                       | 0.00/59.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000001.parquet:  57%|████████      | 31.5M/54.9M [00:00<00:00, 46.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000004.parquet:  96%|█████████████▌| 41.9M/43.5M [00:00<00:00, 52.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000004.parquet: 100%|██████████████| 43.5M/43.5M [00:00<00:00, 50.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000004.parquet\n",
      "\n",
      "\n",
      "\n",
      "episode_000000.parquet:  83%|███████████▋  | 52.4M/63.1M [00:00<00:00, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000006.parquet:  84%|███████████▊  | 52.4M/62.4M [00:00<00:00, 55.7MB/s]\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000010.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/rsLn5BGebIIw3IlGoKoDqdbDfJY=.42ed85fc6f5f1112b9e7359613a9571a8fabfa4d324692a357f99782c443977e.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000010.parquet:   0%|                       | 0.00/31.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000002.parquet:  50%|███████       | 41.9M/83.5M [00:00<00:00, 51.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000008.parquet:  19%|██▋           | 10.5M/54.1M [00:00<00:00, 54.9MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000001.parquet:  76%|██████████▋   | 41.9M/54.9M [00:00<00:00, 51.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000009.parquet:  18%|██▍           | 10.5M/59.2M [00:00<00:00, 49.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000000.parquet: 100%|█████████████▉| 62.9M/63.1M [00:01<00:00, 59.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000000.parquet: 100%|██████████████| 63.1M/63.1M [00:01<00:00, 59.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000000.parquet\n",
      "episode_000007.parquet: 100%|██████████████| 41.9M/41.9M [00:00<00:00, 44.5MB/s]\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000007.parquet\n",
      "Fetching 60 files:   3%|▊                        | 2/60 [00:01<00:34,  1.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "episode_000010.parquet:  34%|████▋         | 10.5M/31.3M [00:00<00:00, 60.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000006.parquet: 100%|██████████████| 62.4M/62.4M [00:01<00:00, 52.8MB/s]\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000011.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/JQ2X4gG6nwj5wZMKNyFm_75pPUk=.d595f724bac49912b7914f9109a66b0f35d169bb8d016b38544e9a135c8891ec.incomplete'\n",
      "episode_000006.parquet: 100%|██████████████| 62.4M/62.4M [00:01<00:00, 56.0MB/s]\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000006.parquet\n",
      "\n",
      "\n",
      "episode_000011.parquet:   0%|                       | 0.00/46.3M [00:00<?, ?B/s]\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000012.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/TPqbhsWoatE4yKHw2We8Qg8XB3E=.d94335651f6d3851a795e77b430b4737471e07066f37900599046ff61bc025eb.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "episode_000012.parquet:   0%|                       | 0.00/52.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000002.parquet:  63%|████████▊     | 52.4M/83.5M [00:01<00:00, 50.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000013.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/P7NzDX_I7S6_SWyl55hM3vUUTys=.c4930d95330a0c23b0bd56d0cee363ec48cbd279c42026129577db9ab9d6ec59.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000013.parquet:   0%|                       | 0.00/49.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000001.parquet:  96%|█████████████▎| 52.4M/54.9M [00:01<00:00, 51.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000008.parquet:  39%|█████▍        | 21.0M/54.1M [00:00<00:00, 45.6MB/s]\u001b[A\n",
      "\n",
      "episode_000011.parquet:  23%|███▏          | 10.5M/46.3M [00:00<00:00, 69.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000001.parquet: 100%|██████████████| 54.9M/54.9M [00:01<00:00, 45.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000001.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fetching 60 files:   5%|█▎                       | 3/60 [00:01<00:25,  2.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000013.parquet:  21%|██▉           | 10.5M/49.3M [00:00<00:00, 66.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000014.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/A0l4hNmU7lRjoEk64OZsJbvpci0=.7bfc60e0e34b293ce6fb88937d5e605c9916d0c0593e1a5333fe51d8c39cc6f5.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000014.parquet:   0%|                       | 0.00/50.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000012.parquet:  20%|██▊           | 10.5M/52.3M [00:00<00:00, 42.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000002.parquet:  75%|██████████▌   | 62.9M/83.5M [00:01<00:00, 50.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000008.parquet:  58%|████████▏     | 31.5M/54.1M [00:00<00:00, 47.0MB/s]\u001b[A\n",
      "\n",
      "episode_000011.parquet:  45%|██████▎       | 21.0M/46.3M [00:00<00:00, 59.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000010.parquet: 100%|██████████████| 31.3M/31.3M [00:00<00:00, 56.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000010.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000013.parquet:  43%|█████▉        | 21.0M/49.3M [00:00<00:00, 62.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000009.parquet:  53%|███████▍      | 31.5M/59.2M [00:00<00:00, 43.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000002.parquet:  88%|████████████▎ | 73.4M/83.5M [00:01<00:00, 55.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000014.parquet:  21%|██▉           | 10.5M/50.4M [00:00<00:00, 59.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000015.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/u8GZgFEljpyIUPL_IlWveQkE4bs=.2b2599775974b0dac93b83f37b2bfd0f8fd1b01bfb739674d39d3724b75b962a.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000015.parquet:   0%|                       | 0.00/53.0M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000008.parquet:  78%|██████████▊   | 41.9M/54.1M [00:00<00:00, 52.6MB/s]\u001b[A\n",
      "\n",
      "\n",
      "episode_000012.parquet:  40%|█████▌        | 21.0M/52.3M [00:00<00:00, 40.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000011.parquet:  68%|█████████▌    | 31.5M/46.3M [00:00<00:00, 58.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000002.parquet: 100%|██████████████| 83.5M/83.5M [00:01<00:00, 52.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000002.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fetching 60 files:   7%|█▋                       | 4/60 [00:01<00:23,  2.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000013.parquet:  64%|████████▉     | 31.5M/49.3M [00:00<00:00, 58.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000015.parquet:  20%|██▊           | 10.5M/53.0M [00:00<00:00, 64.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000014.parquet:  42%|█████▊        | 21.0M/50.4M [00:00<00:00, 56.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000016.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/rQy_R7aepl1p-x8IAbvG49rmwXI=.d73994ecb7f5a6c147e19b125ffaa5eec038d26fe57aa3264ee17ffbfed7277b.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000016.parquet:   0%|                       | 0.00/39.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000008.parquet:  97%|█████████████▌| 52.4M/54.1M [00:01<00:00, 54.3MB/s]\u001b[A\n",
      "\n",
      "episode_000008.parquet: 100%|██████████████| 54.1M/54.1M [00:01<00:00, 51.9MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000008.parquet\n",
      "Fetching 60 files:  17%|████                    | 10/60 [00:01<00:05,  8.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000009.parquet:  89%|████████████▍ | 52.4M/59.2M [00:01<00:00, 54.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000013.parquet:  85%|███████████▉  | 41.9M/49.3M [00:00<00:00, 61.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000015.parquet:  40%|█████▌        | 21.0M/53.0M [00:00<00:00, 66.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000017.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/ozqif_TQP2X1DHgMmtW6yBPBUqo=.60df0249964f1c15cdd20cbea2506d9f27cab9ecfc4b29e0bde19b7821615665.incomplete'\n",
      "\n",
      "episode_000017.parquet:   0%|                       | 0.00/46.2M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000011.parquet: 100%|██████████████| 46.3M/46.3M [00:00<00:00, 58.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000011.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000014.parquet:  62%|████████▋     | 31.5M/50.4M [00:00<00:00, 55.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000012.parquet:  60%|████████▍     | 31.5M/52.3M [00:00<00:00, 37.8MB/s]\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000018.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/6biO3olOD5nJEJQBi-EJJYTOyn4=.0060ce38a2e78e6576c1c54fa5934834d27ceec77fba814d27aa8fe2a9bb798f.incomplete'\n",
      "\n",
      "\n",
      "episode_000018.parquet:   0%|                       | 0.00/40.8M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000009.parquet: 100%|██████████████| 59.2M/59.2M [00:01<00:00, 54.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000009.parquet: 100%|██████████████| 59.2M/59.2M [00:01<00:00, 50.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000009.parquet\n",
      "episode_000013.parquet: 100%|██████████████| 49.3M/49.3M [00:00<00:00, 60.8MB/s]\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000013.parquet\n",
      "Fetching 60 files:  20%|████▊                   | 12/60 [00:02<00:05,  9.26it/s]\n",
      "episode_000017.parquet:  23%|███▏          | 10.5M/46.2M [00:00<00:00, 71.8MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000015.parquet:  59%|████████▎     | 31.5M/53.0M [00:00<00:00, 62.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000016.parquet:  53%|███████▍      | 21.0M/39.4M [00:00<00:00, 65.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000019.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/DicwzzxqerJmAskDiOn4wMk3ozw=.a46ba3992636df5d23a02c21fb5be896481f658ade34e7265cf7433beea469a6.incomplete'\n",
      "Downloading 'data/chunk-000/episode_000020.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/kAnA1JDRWpD44_jOoqBhQvr8yao=.d51212d6a1bb1fae0fbd46fd311432a573b0aff898a30978cdc75e1ac1a1b429.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000019.parquet:   0%|                       | 0.00/57.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000020.parquet:   0%|                       | 0.00/51.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000014.parquet:  83%|███████████▋  | 41.9M/50.4M [00:00<00:00, 55.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000018.parquet:  26%|███▌          | 10.5M/40.8M [00:00<00:00, 56.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000012.parquet:  80%|███████████▏  | 41.9M/52.3M [00:01<00:00, 41.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000019.parquet:  18%|██▌           | 10.5M/57.3M [00:00<00:00, 80.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000017.parquet:  45%|██████▎       | 21.0M/46.2M [00:00<00:00, 57.7MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000015.parquet:  79%|███████████   | 41.9M/53.0M [00:00<00:00, 59.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000016.parquet:  80%|███████████▏  | 31.5M/39.4M [00:00<00:00, 62.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000014.parquet: 100%|██████████████| 50.4M/50.4M [00:00<00:00, 54.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000014.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000020.parquet:  20%|██▊           | 10.5M/51.9M [00:00<00:00, 47.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000021.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/X1vFYaaQ6l2TZRowcx6SWVPcMmE=.1d20d986a7d3073a08e1ebe6e8516547a19f0234d8a0df592347087f1928aa1c.incomplete'\n",
      "episode_000016.parquet: 100%|██████████████| 39.4M/39.4M [00:00<00:00, 66.1MB/s]\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000016.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000021.parquet:   0%|                       | 0.00/33.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000018.parquet:  51%|███████▏      | 21.0M/40.8M [00:00<00:00, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000019.parquet:  37%|█████▏        | 21.0M/57.3M [00:00<00:00, 65.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000015.parquet:  99%|█████████████▊| 52.4M/53.0M [00:00<00:00, 60.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000017.parquet:  68%|█████████▌    | 31.5M/46.2M [00:00<00:00, 58.0MB/s]\u001b[A\n",
      "\n",
      "\n",
      "episode_000015.parquet: 100%|██████████████| 53.0M/53.0M [00:00<00:00, 60.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000015.parquet\n",
      "episode_000012.parquet: 100%|██████████████| 52.3M/52.3M [00:01<00:00, 39.7MB/s]\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000012.parquet\n",
      "Fetching 60 files:  23%|█████▌                  | 14/60 [00:02<00:06,  7.09it/s]Downloading 'data/chunk-000/episode_000022.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/P3usDF3yP4AtWArpIDLVQz6VO3I=.1a67b67d04738cd3183b3a1bba958847df1b99bd806b1088dda31f3dbbe3b823.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "episode_000022.parquet:   0%|                       | 0.00/62.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000020.parquet:  40%|█████▋        | 21.0M/51.9M [00:00<00:00, 52.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000021.parquet:  31%|████▎         | 10.5M/33.7M [00:00<00:00, 64.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000024.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/JQOCHO9v0JM9H6i9avhztzFxn54=.4db04c69eadfb618fbb4a96ccd87526163b98fe6f7c305d33dc38605abb97351.incomplete'\n",
      "Downloading 'data/chunk-000/episode_000023.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/hfUp9qsRjsaz10T0r_kxUJb1PzM=.6c939431f23f04b51d2be3ff1fa514795004c4d7968045bdc356322721d81bf3.incomplete'\n",
      "\n",
      "\n",
      "episode_000018.parquet:  77%|██████████▊   | 31.5M/40.8M [00:00<00:00, 54.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000023.parquet:   0%|                       | 0.00/51.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000024.parquet:   0%|                       | 0.00/30.0M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000019.parquet:  55%|███████▋      | 31.5M/57.3M [00:00<00:00, 62.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000017.parquet: 100%|██████████████| 46.2M/46.2M [00:00<00:00, 60.7MB/s]\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000017.parquet\n",
      "\n",
      "\n",
      "\n",
      "Fetching 60 files:  32%|███████▌                | 19/60 [00:02<00:03, 10.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000020.parquet:  61%|████████▍     | 31.5M/51.9M [00:00<00:00, 51.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000018.parquet: 100%|██████████████| 40.8M/40.8M [00:00<00:00, 54.7MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000018.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000023.parquet:  20%|██▊           | 10.5M/51.8M [00:00<00:00, 63.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000025.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/9UxJgHTg9tsGuPYDdr_du6ahv9I=.018d029e79609fa6cf2bf858f42911338dfe8b2f2d7cd6b1421ce47f7e2928b8.incomplete'\n",
      "\n",
      "episode_000025.parquet:   0%|                       | 0.00/66.3M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000019.parquet:  73%|██████████▎   | 41.9M/57.3M [00:00<00:00, 63.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000021.parquet:  62%|████████▋     | 21.0M/33.7M [00:00<00:00, 54.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000026.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/XqhQ3OuXph8ZzfbQ938Ih0Br1BA=.eebcc91048ba296e5157962989ada5e3a970e5209bab4ccfeb372ff5f3017aaf.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000024.parquet:  35%|████▉         | 10.5M/30.0M [00:00<00:00, 48.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000026.parquet:   0%|                       | 0.00/53.3M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000022.parquet:  33%|████▋         | 21.0M/62.7M [00:00<00:00, 53.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000023.parquet:  41%|█████▋        | 21.0M/51.8M [00:00<00:00, 67.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000020.parquet:  81%|███████████▎  | 41.9M/51.9M [00:00<00:00, 54.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000025.parquet:  16%|██▏           | 10.5M/66.3M [00:00<00:00, 64.4MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000019.parquet:  92%|████████████▊ | 52.4M/57.3M [00:00<00:00, 63.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000021.parquet:  93%|█████████████ | 31.5M/33.7M [00:00<00:00, 58.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000021.parquet: 100%|██████████████| 33.7M/33.7M [00:00<00:00, 58.5MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000021.parquet\n",
      "episode_000019.parquet: 100%|██████████████| 57.3M/57.3M [00:00<00:00, 65.2MB/s]\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000019.parquet\n",
      "Fetching 60 files:  35%|████████▍               | 21/60 [00:03<00:04,  9.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000024.parquet:  70%|█████████▊    | 21.0M/30.0M [00:00<00:00, 48.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000027.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/oghbAfzL0dQC8G9LIkthTnv6kKs=.c47b1d188ec746fdc6a39f7f964981d76cd1c335726cc06af00043894a7906d2.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000020.parquet: 100%|██████████████| 51.9M/51.9M [00:00<00:00, 61.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000027.parquet:   0%|                       | 0.00/27.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000028.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/Qt39NVoiIe3SlqER6VbVFooiBUI=.27afc82b0379c8a50a42a232702409b1e68e8c5c0f5d5772bdc8f248a9695bd8.incomplete'\n",
      "episode_000020.parquet: 100%|██████████████| 51.9M/51.9M [00:00<00:00, 56.2MB/s]\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000020.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000028.parquet:   0%|                       | 0.00/43.0M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000022.parquet:  50%|███████       | 31.5M/62.7M [00:00<00:00, 58.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000023.parquet:  61%|████████▌     | 31.5M/51.8M [00:00<00:00, 64.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000029.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/opdxwt05yqhVvDzrD6x5U-Z9mCk=.fb706ecba9317c3f7a7292c90e6972b2ae371c08c01b9230389a1d4a019c7706.incomplete'\n",
      "\n",
      "episode_000025.parquet:  32%|████▍         | 21.0M/66.3M [00:00<00:00, 55.6MB/s]\u001b[A\n",
      "\n",
      "episode_000026.parquet:  39%|█████▌        | 21.0M/53.3M [00:00<00:00, 67.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000029.parquet:   0%|                       | 0.00/55.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000024.parquet: 100%|██████████████| 30.0M/30.0M [00:00<00:00, 49.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000024.parquet\n",
      "\n",
      "\n",
      "\n",
      "episode_000022.parquet:  67%|█████████▎    | 41.9M/62.7M [00:00<00:00, 60.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000023.parquet:  81%|███████████▎  | 41.9M/51.8M [00:00<00:00, 66.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000028.parquet:  24%|███▍          | 10.5M/43.0M [00:00<00:00, 50.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000030.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/bUdkaBS7BCUc894SOOEjCPeiu2Q=.a8a5ae2f437e18a44189ff45e64242047c81bb82dcac69a2969aef080a4be758.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000027.parquet:  38%|█████▎        | 10.5M/27.6M [00:00<00:00, 44.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000030.parquet:   0%|                       | 0.00/64.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000026.parquet:  59%|████████▎     | 31.5M/53.3M [00:00<00:00, 66.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000029.parquet:  19%|██▌           | 10.5M/55.9M [00:00<00:00, 53.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000025.parquet:  47%|██████▋       | 31.5M/66.3M [00:00<00:00, 51.0MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000023.parquet: 100%|██████████████| 51.8M/51.8M [00:00<00:00, 65.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000023.parquet\n",
      "\n",
      "\n",
      "\n",
      "episode_000022.parquet:  84%|███████████▋  | 52.4M/62.7M [00:00<00:00, 61.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000030.parquet:  16%|██▎           | 10.5M/64.4M [00:00<00:00, 69.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000031.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/H1Om1Sn5gSm9_wavda5WgWxdVqM=.093d2e50f7be947082d217a8a82a126b87ebd1caf8d0bc3badf20b7114b167ce.incomplete'\n",
      "\n",
      "\n",
      "episode_000026.parquet:  79%|███████████   | 41.9M/53.3M [00:00<00:00, 67.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000027.parquet:  76%|██████████▋   | 21.0M/27.6M [00:00<00:00, 51.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000031.parquet:   0%|                       | 0.00/58.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000028.parquet:  49%|██████▊       | 21.0M/43.0M [00:00<00:00, 43.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000022.parquet: 100%|██████████████| 62.7M/62.7M [00:01<00:00, 59.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000022.parquet\n",
      "Fetching 60 files:  40%|█████████▌              | 24/60 [00:03<00:04,  7.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000027.parquet: 100%|██████████████| 27.6M/27.6M [00:00<00:00, 54.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000027.parquet: 100%|██████████████| 27.6M/27.6M [00:00<00:00, 50.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000027.parquet\n",
      "\n",
      "episode_000025.parquet:  63%|████████▊     | 41.9M/66.3M [00:00<00:00, 48.5MB/s]\u001b[ADownloading 'data/chunk-000/episode_000032.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/UawB5YWb8ss0olgzpV7nWf33qgI=.5da6d8042028c8a9af55dc7c52d222ad2fcd4e6f7eb69d2287919fc0d6c3a42f.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000031.parquet:  18%|██▌           | 10.5M/58.2M [00:00<00:00, 70.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000032.parquet:   0%|                       | 0.00/41.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000026.parquet:  98%|█████████████▊| 52.4M/53.3M [00:00<00:00, 64.3MB/s]\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000033.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/ymTADZxPQlMdlZGofS8XPs5P8Ko=.45a1740719a2822b573482dba62fe4e98bf6150749c5d31a356b7aba4b916730.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000026.parquet: 100%|██████████████| 53.3M/53.3M [00:00<00:00, 64.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000026.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000030.parquet:  33%|████▌         | 21.0M/64.4M [00:00<00:00, 52.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000034.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/W7uphJ2IDhKHSBDC2uw5rsWy9lM=.99b659771f5bc3b3edd04e3d0c50fb07845335477afdf5fcf07bd0f26de0819c.incomplete'\n",
      "\n",
      "\n",
      "episode_000034.parquet:   0%|                       | 0.00/38.5M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000028.parquet:  73%|██████████▏   | 31.5M/43.0M [00:00<00:00, 44.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000025.parquet:  79%|███████████   | 52.4M/66.3M [00:01<00:00, 51.1MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000031.parquet:  36%|█████         | 21.0M/58.2M [00:00<00:00, 62.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000029.parquet:  56%|███████▊      | 31.5M/55.9M [00:00<00:00, 46.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000033.parquet:  26%|███▋          | 10.5M/40.2M [00:00<00:00, 55.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000032.parquet:  25%|███▌          | 10.5M/41.7M [00:00<00:00, 41.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000030.parquet:  49%|██████▊       | 31.5M/64.4M [00:00<00:00, 48.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000034.parquet:  27%|███▊          | 10.5M/38.5M [00:00<00:00, 59.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000028.parquet:  98%|█████████████▋| 41.9M/43.0M [00:00<00:00, 48.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000028.parquet: 100%|██████████████| 43.0M/43.0M [00:00<00:00, 46.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000028.parquet\n",
      "\n",
      "episode_000025.parquet:  95%|█████████████▎| 62.9M/66.3M [00:01<00:00, 51.2MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000029.parquet:  75%|██████████▍   | 41.9M/55.9M [00:00<00:00, 48.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000035.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/yRlyCAKuLEPSJPhpEToPK0m2K9A=.5ac41556a12b2d479946a6de70b74161a0e9d7381e17ee209df37e3496bd0413.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000035.parquet:   0%|                       | 0.00/47.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000025.parquet: 100%|██████████████| 66.3M/66.3M [00:01<00:00, 51.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000025.parquet\n",
      "Fetching 60 files:  45%|██████████▊             | 27/60 [00:04<00:04,  7.03it/s]\n",
      "\n",
      "episode_000034.parquet:  54%|███████▌      | 21.0M/38.5M [00:00<00:00, 61.0MB/s]\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000036.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/KNUYHjLT4Jgk8Othn7hoIeFxFAo=.cd03c35071d6d291684a26fee5760bc9b2fab9fcdc6993ea46b97cbd2650c6d8.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "episode_000032.parquet:  50%|███████       | 21.0M/41.7M [00:00<00:00, 41.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "episode_000036.parquet:   0%|                       | 0.00/39.6M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000031.parquet:  72%|██████████    | 41.9M/58.2M [00:00<00:00, 61.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000030.parquet:  65%|█████████     | 41.9M/64.4M [00:00<00:00, 45.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000029.parquet:  94%|█████████████ | 52.4M/55.9M [00:01<00:00, 49.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000033.parquet:  78%|██████████▉   | 31.5M/40.2M [00:00<00:00, 53.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000035.parquet:  22%|███           | 10.5M/47.9M [00:00<00:00, 43.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000029.parquet: 100%|██████████████| 55.9M/55.9M [00:01<00:00, 48.5MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000029.parquet\n",
      "\n",
      "Fetching 60 files:  52%|████████████▍           | 31/60 [00:04<00:03,  8.96it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000031.parquet:  90%|████████████▌ | 52.4M/58.2M [00:00<00:00, 60.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000030.parquet:  81%|███████████▍  | 52.4M/64.4M [00:01<00:00, 48.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000033.parquet: 100%|██████████████| 40.2M/40.2M [00:00<00:00, 56.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000033.parquet\n",
      "\n",
      "\n",
      "\n",
      "episode_000032.parquet:  75%|██████████▌   | 31.5M/41.7M [00:00<00:00, 42.7MB/s]\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000037.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/qoxNvU6W4JflWiwgID4mn8vj7vI=.7b186234f826f8728a7559183da85be73cf6369eb8cf3e48e01e2bd5b72e18ce.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000037.parquet:   0%|                       | 0.00/43.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000034.parquet: 100%|██████████████| 38.5M/38.5M [00:00<00:00, 57.6MB/s]\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000038.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/H1Kgl9KCydxJM7ZLyrt78HmE_Nc=.49c579816f3d45bad59bf212371056cf1a7bd752459e77675e80e355cd1dc34a.incomplete'\n",
      "episode_000034.parquet: 100%|██████████████| 38.5M/38.5M [00:00<00:00, 57.9MB/s]\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000034.parquet\n",
      "\n",
      "\n",
      "episode_000031.parquet: 100%|██████████████| 58.2M/58.2M [00:00<00:00, 60.2MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000031.parquet\n",
      "\n",
      "episode_000036.parquet:  53%|███████▍      | 21.0M/39.6M [00:00<00:00, 62.5MB/s]\u001b[ADownloading 'data/chunk-000/episode_000039.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/f5E5idQanWFATMapRx5VTTSnBqU=.848855eef4ae4edb85cd70d77ee0e87a4c9ee156a4a458116e221e20a81d5c65.incomplete'\n",
      "Downloading 'data/chunk-000/episode_000040.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/LERN9bLN5OxzA7e38s8SRgaZ4wI=.55e532986966f7ebfd24584babf179695c4840eb6e84587b4a406dd9a9ac4a82.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000039.parquet:   0%|                       | 0.00/43.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000040.parquet:   0%|                       | 0.00/62.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000032.parquet: 100%|██████████████| 41.7M/41.7M [00:00<00:00, 45.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000032.parquet\n",
      "\n",
      "\n",
      "episode_000038.parquet:  21%|██▉           | 10.5M/50.3M [00:00<00:00, 74.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000030.parquet:  98%|█████████████▋| 62.9M/64.4M [00:01<00:00, 48.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000035.parquet:  44%|██████▏       | 21.0M/47.9M [00:00<00:00, 37.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000037.parquet:  24%|███▎          | 10.5M/43.6M [00:00<00:00, 51.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000041.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/vFu-uzdC1JKhrqKqqy4fRwczirM=.7a4999019b52e70556406a087056b984e55a8eebc605a56cb8bdd830604a8492.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "episode_000030.parquet: 100%|██████████████| 64.4M/64.4M [00:01<00:00, 48.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000030.parquet\n",
      "Fetching 60 files:  55%|█████████████▏          | 33/60 [00:04<00:03,  8.20it/s]\n",
      "episode_000036.parquet:  80%|███████████▏  | 31.5M/39.6M [00:00<00:00, 60.1MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading 'data/chunk-000/episode_000042.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/_Z6wnQAAWrf_PpgSxxqWp0FLeU0=.9297c8860ba2fae3905c491c702b88a908279a95adfcecc2a75f299b1eabe5fa.incomplete'\n",
      "episode_000040.parquet:  17%|██▎           | 10.5M/62.5M [00:00<00:00, 55.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000042.parquet:   0%|                       | 0.00/46.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000038.parquet:  42%|█████▊        | 21.0M/50.3M [00:00<00:00, 60.2MB/s]\u001b[A\u001b[A\n",
      "episode_000036.parquet: 100%|██████████████| 39.6M/39.6M [00:00<00:00, 59.7MB/s]\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000036.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000037.parquet:  48%|██████▋       | 21.0M/43.6M [00:00<00:00, 52.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000039.parquet:  24%|███▎          | 10.5M/43.7M [00:00<00:01, 31.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000041.parquet:  12%|█▌            | 10.5M/91.2M [00:00<00:01, 44.5MB/s]\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000043.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/ybE89QBa5jrD55ZYXmRD413o5Mg=.478e9ce79df0db534e3bdb08d35904238cf11c88260724fe8f78c1c242369ad8.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000035.parquet:  66%|█████████▏    | 31.5M/47.9M [00:00<00:00, 37.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000043.parquet:   0%|                       | 0.00/48.5M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000040.parquet:  34%|████▋         | 21.0M/62.5M [00:00<00:00, 51.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000038.parquet:  63%|████████▊     | 31.5M/50.3M [00:00<00:00, 54.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000042.parquet:  23%|███▏          | 10.5M/46.3M [00:00<00:00, 38.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000037.parquet:  72%|██████████    | 31.5M/43.6M [00:00<00:00, 52.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000041.parquet:  23%|███▏          | 21.0M/91.2M [00:00<00:01, 53.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000035.parquet:  88%|████████████▎ | 41.9M/47.9M [00:01<00:00, 42.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000043.parquet:  22%|███           | 10.5M/48.5M [00:00<00:00, 54.6MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000040.parquet:  50%|███████       | 31.5M/62.5M [00:00<00:00, 53.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000035.parquet: 100%|██████████████| 47.9M/47.9M [00:01<00:00, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000035.parquet\n",
      "Fetching 60 files:  62%|██████████████▊         | 37/60 [00:05<00:02,  7.96it/s]\n",
      "\n",
      "episode_000038.parquet:  83%|███████████▋  | 41.9M/50.3M [00:00<00:00, 54.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000037.parquet:  96%|█████████████▍| 41.9M/43.6M [00:00<00:00, 55.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000037.parquet: 100%|██████████████| 43.6M/43.6M [00:00<00:00, 53.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000037.parquet\n",
      "\n",
      "episode_000043.parquet:  43%|██████        | 21.0M/48.5M [00:00<00:00, 60.8MB/s]\u001b[ADownloading 'data/chunk-000/episode_000044.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/jQ39IQnXI7WI6dA7gdrs8FLzqTM=.1dde7a452c1dd9e89be01718e580db4be2b77f41f966cb019d497a216f2fa7c9.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000044.parquet:   0%|                       | 0.00/53.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000045.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/4xUDU2aUJLBTyr_bVKPgOc1HEPE=.1edad6e077dffe25155350aa065ae710d063a2578b3f5e602c31a8b46db31317.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000040.parquet:  67%|█████████▍    | 41.9M/62.5M [00:00<00:00, 53.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000038.parquet: 100%|██████████████| 50.3M/50.3M [00:00<00:00, 58.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000042.parquet:  45%|██████▎       | 21.0M/46.3M [00:00<00:00, 35.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000038.parquet: 100%|██████████████| 50.3M/50.3M [00:00<00:00, 57.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000038.parquet\n",
      "\n",
      "\n",
      "Fetching 60 files:  67%|████████████████        | 40/60 [00:05<00:02,  9.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000041.parquet:  46%|██████▍       | 41.9M/91.2M [00:00<00:00, 62.2MB/s]\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000046.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/MbL1mizwru_lBWVlmUHhPtzY260=.4ca4212dbb4a8f9d150dbae9e9f0cacd3c89b916a42f3e6b25f0273064adc93b.incomplete'\n",
      "\n",
      "episode_000043.parquet:  65%|█████████     | 31.5M/48.5M [00:00<00:00, 64.2MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000046.parquet:   0%|                       | 0.00/60.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000045.parquet:  28%|███▉          | 10.5M/37.0M [00:00<00:00, 67.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000041.parquet:  58%|████████      | 52.4M/91.2M [00:00<00:00, 65.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000042.parquet:  68%|█████████▌    | 31.5M/46.3M [00:00<00:00, 42.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000040.parquet:  84%|███████████▊  | 52.4M/62.5M [00:00<00:00, 53.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000043.parquet:  86%|████████████  | 41.9M/48.5M [00:00<00:00, 69.1MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000039.parquet:  96%|█████████████▍| 41.9M/43.7M [00:01<00:00, 44.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000039.parquet: 100%|██████████████| 43.7M/43.7M [00:01<00:00, 40.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000039.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000046.parquet:  17%|██▍           | 10.5M/60.2M [00:00<00:00, 56.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000047.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/2qKOI5uj8Zquao3OfLkgepEAWrQ=.156a52ca3c8c4c0d26376570b1b2fb6090dda492d415aad5ee9dcb82bb884ec8.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000043.parquet: 100%|██████████████| 48.5M/48.5M [00:00<00:00, 62.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000043.parquet\n",
      "\n",
      "\n",
      "episode_000045.parquet:  57%|███████▉      | 21.0M/37.0M [00:00<00:00, 57.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000041.parquet:  69%|█████████▋    | 62.9M/91.2M [00:01<00:00, 60.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000040.parquet: 100%|██████████████| 62.5M/62.5M [00:01<00:00, 52.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000048.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/o1Y0RqALqdcVwbQhhM_cwvB3x8k=.460ee432a58c69f73374342bbcf7a25e37c3de4487dab0961a38b05a60095079.incomplete'\n",
      "episode_000040.parquet: 100%|██████████████| 62.5M/62.5M [00:01<00:00, 52.4MB/s]\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000040.parquet\n",
      "\n",
      "Fetching 60 files:  70%|████████████████▊       | 42/60 [00:05<00:02,  8.10it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000042.parquet:  91%|████████████▋ | 41.9M/46.3M [00:01<00:00, 43.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000049.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/Mww57qQlIfiImOZq7cSV1ebG9Q0=.75f669bb01e7a177ee7cdc2ba354f7d70b967056b1b2bfd7483aba3d7b8d7423.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000047.parquet:  22%|███           | 10.5M/48.2M [00:00<00:00, 69.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000049.parquet:   0%|                       | 0.00/25.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000046.parquet:  35%|████▉         | 21.0M/60.2M [00:00<00:00, 54.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000042.parquet: 100%|██████████████| 46.3M/46.3M [00:01<00:00, 40.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000042.parquet\n",
      "\n",
      "\n",
      "episode_000045.parquet:  85%|███████████▉  | 31.5M/37.0M [00:00<00:00, 55.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000041.parquet:  81%|███████████▎  | 73.4M/91.2M [00:01<00:00, 58.5MB/s]\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000050.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/PqpfG_scOuFM1G_UVIEq_r_QZ4g=.ef64842d446a0145280d433a190ba302770bc8eabf712318d29511c0ae51a0af.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000050.parquet:   0%|                       | 0.00/58.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000048.parquet:  25%|███▍          | 10.5M/42.1M [00:00<00:00, 51.7MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000049.parquet:  41%|█████▊        | 10.5M/25.5M [00:00<00:00, 67.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000047.parquet:  43%|██████        | 21.0M/48.2M [00:00<00:00, 61.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000045.parquet: 100%|██████████████| 37.0M/37.0M [00:00<00:00, 55.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000045.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000044.parquet:  58%|████████▏     | 31.5M/53.8M [00:00<00:00, 42.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000041.parquet:  92%|████████████▉ | 83.9M/91.2M [00:01<00:00, 59.4MB/s]\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000051.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/C4Ih8d9pBXzDrkh3FuR4O_6EDaA=.c0c933e777a59e6475eacc1103de1a2fa21390043bddbc1101eb687b6edd7649.incomplete'\n",
      "\n",
      "\n",
      "episode_000051.parquet:   0%|                       | 0.00/42.0M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000049.parquet:  82%|███████████▌  | 21.0M/25.5M [00:00<00:00, 63.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000048.parquet:  50%|██████▉       | 21.0M/42.1M [00:00<00:00, 51.3MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000047.parquet:  65%|█████████▏    | 31.5M/48.2M [00:00<00:00, 61.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000049.parquet: 100%|██████████████| 25.5M/25.5M [00:00<00:00, 63.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000049.parquet\n",
      "\n",
      "\n",
      "\n",
      "episode_000041.parquet: 100%|██████████████| 91.2M/91.2M [00:01<00:00, 58.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000041.parquet: 100%|██████████████| 91.2M/91.2M [00:01<00:00, 58.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000041.parquet\n",
      "Fetching 60 files:  73%|█████████████████▌      | 44/60 [00:06<00:02,  6.63it/s]Downloading 'data/chunk-000/episode_000052.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/XAZsM6mfNfwU_V4rxHeuGab8xFU=.a0596cb5fd5c1311e4330bf467d10c89196660ed7cad6017e1072919a4f82655.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000050.parquet:  18%|██▌           | 10.5M/58.1M [00:00<00:01, 32.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000052.parquet:   0%|                       | 0.00/65.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000053.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/4x8gNUHI7JuyMCARm7Q-5YnKcNo=.ed67a72652e19917f69069789081a9bed0d9e52e2c01c2349cdf873f5dc6d185.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000053.parquet:   0%|                       | 0.00/39.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000048.parquet:  75%|██████████▍   | 31.5M/42.1M [00:00<00:00, 52.2MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000047.parquet:  87%|████████████▏ | 41.9M/48.2M [00:00<00:00, 58.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000051.parquet:  25%|███▍          | 10.5M/42.0M [00:00<00:00, 39.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000046.parquet:  87%|████████████▏ | 52.4M/60.2M [00:00<00:00, 56.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000044.parquet: 100%|██████████████| 53.8M/53.8M [00:01<00:00, 45.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000044.parquet\n",
      "Fetching 60 files:  77%|██████████████████▍     | 46/60 [00:06<00:01,  7.07it/s]\n",
      "\n",
      "\n",
      "\n",
      "episode_000047.parquet: 100%|██████████████| 48.2M/48.2M [00:00<00:00, 59.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000047.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000053.parquet:  27%|███▊          | 10.5M/39.1M [00:00<00:00, 50.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'data/chunk-000/episode_000054.parquet' to 'PnPCounterToSink/.cache/huggingface/download/data/chunk-000/HWJ8on9SxqmPPWqi7CczyilJwAw=.b1b6e036ac37c1008ba3a018cdc1a9262b8aaa3472e1be4300b61b46b08a2153.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000054.parquet:   0%|                       | 0.00/73.0M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'meta/episodes.jsonl' to 'PnPCounterToSink/.cache/huggingface/download/meta/Xr2m2iHl-ydc-mHHfOvaRU5pi6o=.830cf9b8a2c2ba496d32990e81c5227fc519a936.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "episode_000052.parquet:  16%|██▏           | 10.5M/65.4M [00:00<00:01, 41.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episodes.jsonl: 6.07kB [00:00, 7.70MB/s]A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/meta/episodes.jsonl\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000046.parquet: 100%|██████████████| 60.2M/60.2M [00:01<00:00, 54.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000046.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fetching 60 files:  80%|███████████████████▏    | 48/60 [00:06<00:01,  8.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "episode_000048.parquet: 100%|█████████████▉| 41.9M/42.1M [00:00<00:00, 50.0MB/s]\u001b[ADownloading 'meta/episodes_stats.jsonl' to 'PnPCounterToSink/.cache/huggingface/download/meta/OSV6LZWLsNGAwnFDfYELN9vNGMw=.2fd71f191661381d67bb167896c5b44ccc52122d.incomplete'\n",
      "episode_000048.parquet: 100%|██████████████| 42.1M/42.1M [00:00<00:00, 49.7MB/s]\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000048.parquet\n",
      "\n",
      "episodes_stats.jsonl: 142kB [00:00, 31.5MB/s]\n",
      "Download complete. Moving file to PnPCounterToSink/meta/episodes_stats.jsonl\n",
      "Downloading 'meta/info.json' to 'PnPCounterToSink/.cache/huggingface/download/meta/c-Q9HGFI0JpMTOZ5pylhXwaYu2U=.8d8b796860e2c92a2571c75cc126a306ce4162e2.incomplete'\n",
      "\n",
      "info.json: 2.35kB [00:00, 3.90MB/s]\n",
      "Download complete. Moving file to PnPCounterToSink/meta/info.json\n",
      "\n",
      "\n",
      "episode_000051.parquet:  50%|██████▉       | 21.0M/42.0M [00:00<00:00, 40.1MB/s]\u001b[A\u001b[ADownloading 'meta/tasks.jsonl' to 'PnPCounterToSink/.cache/huggingface/download/meta/NzWDcOlnTQhMC9idVZndPYjQOjQ=.7278c2dd2f9ca67113778fb91fcb881d59eb30ca.incomplete'\n",
      "\n",
      "tasks.jsonl: 2.85kB [00:00, 3.71MB/s]\n",
      "Download complete. Moving file to PnPCounterToSink/meta/tasks.jsonl\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000054.parquet:  14%|██            | 10.5M/73.0M [00:00<00:01, 61.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000053.parquet:  54%|███████▌      | 21.0M/39.1M [00:00<00:00, 49.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "episode_000052.parquet:  32%|████▍         | 21.0M/65.4M [00:00<00:00, 48.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "episode_000051.parquet:  75%|██████████▍   | 31.5M/42.0M [00:00<00:00, 51.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000050.parquet:  54%|███████▌      | 31.5M/58.1M [00:00<00:00, 39.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000054.parquet:  29%|████          | 21.0M/73.0M [00:00<00:00, 74.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000051.parquet: 100%|██████████████| 42.0M/42.0M [00:00<00:00, 56.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000051.parquet\n",
      "\n",
      "\n",
      "\n",
      "episode_000053.parquet: 100%|██████████████| 39.1M/39.1M [00:00<00:00, 66.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000053.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000050.parquet:  90%|████████████▋ | 52.4M/58.1M [00:00<00:00, 67.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "episode_000050.parquet: 100%|██████████████| 58.1M/58.1M [00:01<00:00, 56.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000050.parquet\n",
      "Fetching 60 files:  87%|████████████████████▊   | 52/60 [00:06<00:00,  9.01it/s]\n",
      "\n",
      "\n",
      "episode_000052.parquet: 100%|██████████████| 65.4M/65.4M [00:00<00:00, 92.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000052.parquet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode_000054.parquet: 100%|███████████████| 73.0M/73.0M [00:00<00:00, 137MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to PnPCounterToSink/data/chunk-000/episode_000054.parquet\n",
      "Fetching 60 files: 100%|████████████████████████| 60/60 [00:07<00:00,  8.51it/s]\n",
      "/home/ubuntu/hyperstack_smolvla/PnPCounterToSink\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download Lava8888/PnPCounterToSink --repo-type=dataset --local-dir ./PnPCounterToSink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b5e8f6",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4da06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "# If you have your API key, uncomment and set it below:\n",
    "os.environ[\"WANDB_API_KEY\"] = \"2b7ce8448972065134994936e1d777d3674ea828\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "            hub_api = HfApi()\n",
    "            hub_api.create_tag(\"lava8888/PnpCounterToSink\", tag=\"_version_\", repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b594ec2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "409 Client Error: Conflict for url: https://huggingface.co/api/datasets/Lava8888/PnPCounterToSink/tag/main (Request ID: Root=1-6899ff97-6392632c6ec1659279d0d51c;64ecaf5c-97be-4ad9-85a0-4ffbbdbcecd2)\n\nTag reference exists already",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 409 Client Error: Conflict for url: https://huggingface.co/api/datasets/Lava8888/PnPCounterToSink/tag/main",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfApi\n\u001b[1;32m      3\u001b[0m hub_api \u001b[38;5;241m=\u001b[39m HfApi()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mhub_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlava8888/PnpCounterToSink\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_version_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/hf_api.py:6120\u001b[0m, in \u001b[0;36mHfApi.create_tag\u001b[0;34m(self, repo_id, tag, tag_message, revision, token, repo_type, exist_ok)\u001b[0m\n\u001b[1;32m   6118\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mpost(url\u001b[38;5;241m=\u001b[39mtag_url, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mpayload)\n\u001b[1;32m   6119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 6120\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HfHubHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   6122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m exist_ok):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 409 Client Error: Conflict for url: https://huggingface.co/api/datasets/Lava8888/PnPCounterToSink/tag/main (Request ID: Root=1-6899ff97-6392632c6ec1659279d0d51c;64ecaf5c-97be-4ad9-85a0-4ffbbdbcecd2)\n\nTag reference exists already"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "hub_api = HfApi()\n",
    "hub_api.create_tag(\"lava8888/PnpCounterToSink\", tag=\"_version_\", repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac6f487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ~/.cache/huggingface/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b6a952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-08-11 14:41:57 in_model.py:111 {'batch_size': 350,\n",
      " 'dataset': {'episodes': None,\n",
      "             'image_transforms': {'enable': False,\n",
      "                                  'max_num_transforms': 3,\n",
      "                                  'random_order': False,\n",
      "                                  'tfs': {'brightness': {'kwargs': {'brightness': [0.8,\n",
      "                                                                                   1.2]},\n",
      "                                                         'type': 'ColorJitter',\n",
      "                                                         'weight': 1.0},\n",
      "                                          'contrast': {'kwargs': {'contrast': [0.8,\n",
      "                                                                               1.2]},\n",
      "                                                       'type': 'ColorJitter',\n",
      "                                                       'weight': 1.0},\n",
      "                                          'hue': {'kwargs': {'hue': [-0.05,\n",
      "                                                                     0.05]},\n",
      "                                                  'type': 'ColorJitter',\n",
      "                                                  'weight': 1.0},\n",
      "                                          'saturation': {'kwargs': {'saturation': [0.5,\n",
      "                                                                                   1.5]},\n",
      "                                                         'type': 'ColorJitter',\n",
      "                                                         'weight': 1.0},\n",
      "                                          'sharpness': {'kwargs': {'sharpness': [0.5,\n",
      "                                                                                 1.5]},\n",
      "                                                        'type': 'SharpnessJitter',\n",
      "                                                        'weight': 1.0}}},\n",
      "             'repo_id': 'lava8888/PnPCounterToSink',\n",
      "             'revision': None,\n",
      "             'root': './PnPCounterToSink',\n",
      "             'use_imagenet_stats': True,\n",
      "             'video_backend': 'torchcodec'},\n",
      " 'env': None,\n",
      " 'eval': {'batch_size': 50, 'n_episodes': 50, 'use_async_envs': False},\n",
      " 'eval_freq': 5,\n",
      " 'job_name': 'smolvla_6',\n",
      " 'log_freq': 1,\n",
      " 'num_workers': 24,\n",
      " 'optimizer': {'betas': [0.9, 0.95],\n",
      "               'eps': 1e-08,\n",
      "               'grad_clip_norm': 10,\n",
      "               'lr': 0.0001,\n",
      "               'type': 'adamw',\n",
      "               'weight_decay': 1e-10},\n",
      " 'output_dir': 'ckpt/smolvla_omy',\n",
      " 'policy': {'adapt_to_pi_aloha': False,\n",
      "            'add_image_special_tokens': False,\n",
      "            'attention_mode': 'cross_attn',\n",
      "            'chunk_size': 5,\n",
      "            'device': 'cuda',\n",
      "            'empty_cameras': 0,\n",
      "            'expert_width_multiplier': 0.75,\n",
      "            'freeze_vision_encoder': True,\n",
      "            'input_features': {},\n",
      "            'load_vlm_weights': False,\n",
      "            'max_action_dim': 32,\n",
      "            'max_period': 4.0,\n",
      "            'max_state_dim': 32,\n",
      "            'min_period': 0.004,\n",
      "            'n_action_steps': 5,\n",
      "            'n_obs_steps': 1,\n",
      "            'normalization_mapping': {'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
      "                                      'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
      "                                      'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>},\n",
      "            'num_expert_layers': -1,\n",
      "            'num_steps': 10,\n",
      "            'num_vlm_layers': 16,\n",
      "            'optimizer_betas': [0.9, 0.95],\n",
      "            'optimizer_eps': 1e-08,\n",
      "            'optimizer_grad_clip_norm': 10,\n",
      "            'optimizer_lr': 0.0001,\n",
      "            'optimizer_weight_decay': 1e-10,\n",
      "            'output_features': {},\n",
      "            'pad_language_to': 'longest',\n",
      "            'prefix_length': -1,\n",
      "            'resize_imgs_with_padding': [512, 512],\n",
      "            'scheduler_decay_lr': 2.5e-06,\n",
      "            'scheduler_decay_steps': 30000,\n",
      "            'scheduler_warmup_steps': 1000,\n",
      "            'self_attn_every_n_layers': 2,\n",
      "            'tokenizer_max_length': 48,\n",
      "            'train_expert_only': True,\n",
      "            'train_state_proj': True,\n",
      "            'type': 'smolvla',\n",
      "            'use_amp': False,\n",
      "            'use_cache': True,\n",
      "            'use_delta_joint_actions_aloha': False,\n",
      "            'vlm_model_name': 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct'},\n",
      " 'resume': False,\n",
      " 'save_checkpoint': True,\n",
      " 'save_freq': 50,\n",
      " 'scheduler': {'decay_lr': 2.5e-06,\n",
      "               'num_decay_steps': 30000,\n",
      "               'num_warmup_steps': 1000,\n",
      "               'peak_lr': 0.0001,\n",
      "               'type': 'cosine_decay_with_warmup'},\n",
      " 'seed': 42,\n",
      " 'steps': 1000,\n",
      " 'use_policy_training_preset': True,\n",
      " 'wandb': {'disable_artifact': True,\n",
      "           'enable': True,\n",
      "           'entity': 'qualiastudios',\n",
      "           'mode': 'online',\n",
      "           'notes': 'first',\n",
      "           'project': 'smolVLA',\n",
      "           'run_id': '12'}}\n",
      "\u001b[1m\u001b[34mLogs will be synced with wandb.\u001b[0m\n",
      "INFO 2025-08-11 14:41:58 ndb_utils.py:96 Track this run --> \u001b[1m\u001b[33mhttps://wandb.ai/qualiastudios/SmolVLA/runs/12\u001b[0m\n",
      "INFO 2025-08-11 14:41:58 in_model.py:127 Creating dataset\n",
      "Resolving data files: 100%|█████████████████| 55/55 [00:00<00:00, 205786.55it/s]\n",
      "INFO 2025-08-11 14:41:59 in_model.py:138 Creating policy\n",
      "Reducing the number of VLM layers to 16 ...\n",
      "INFO 2025-08-11 14:42:18 in_model.py:148 Creating optimizer and scheduler\n",
      "INFO 2025-08-11 14:42:18 in_model.py:160 \u001b[1m\u001b[33mOutput dir:\u001b[0m ckpt/smolvla_omy\n",
      "INFO 2025-08-11 14:42:18 in_model.py:163 cfg.steps=1000 (1K)\n",
      "INFO 2025-08-11 14:42:18 in_model.py:164 dataset.num_frames=23190 (23K)\n",
      "INFO 2025-08-11 14:42:18 in_model.py:165 dataset.num_episodes=55\n",
      "INFO 2025-08-11 14:42:18 in_model.py:166 num_learnable_params=99880992 (100M)\n",
      "INFO 2025-08-11 14:42:18 in_model.py:167 num_total_params=450046236 (450M)\n",
      "INFO 2025-08-11 14:42:18 in_model.py:206 Start offline training on a fixed dataset\n",
      "Step 0 of 1000\n",
      "INFO 2025-08-11 14:42:53 in_model.py:238 step:1 smpl:350 ep:1 epch:0.02 loss:1.502 grdn:5.938 lr:2.0e-07 updt_s:12.819 data_s:22.430\n",
      "WARNING 2025-08-11 14:42:53 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:42:53 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:42:59 in_model.py:238 step:2 smpl:700 ep:2 epch:0.03 loss:1.556 grdn:6.208 lr:3.0e-07 updt_s:5.509 data_s:0.001\n",
      "WARNING 2025-08-11 14:42:59 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:42:59 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:43:05 in_model.py:238 step:3 smpl:1K ep:2 epch:0.05 loss:1.537 grdn:6.261 lr:4.0e-07 updt_s:6.127 data_s:0.001\n",
      "WARNING 2025-08-11 14:43:05 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:43:05 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:43:11 in_model.py:238 step:4 smpl:1K ep:3 epch:0.06 loss:1.528 grdn:6.128 lr:5.0e-07 updt_s:6.132 data_s:0.000\n",
      "WARNING 2025-08-11 14:43:11 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:43:11 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:43:17 in_model.py:238 step:5 smpl:2K ep:4 epch:0.08 loss:1.545 grdn:6.075 lr:6.0e-07 updt_s:6.169 data_s:0.001\n",
      "WARNING 2025-08-11 14:43:17 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:43:17 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:43:22 in_model.py:238 step:6 smpl:2K ep:5 epch:0.09 loss:1.534 grdn:6.273 lr:7.0e-07 updt_s:4.639 data_s:0.001\n",
      "WARNING 2025-08-11 14:43:22 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:43:22 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:43:27 in_model.py:238 step:7 smpl:2K ep:6 epch:0.11 loss:1.501 grdn:6.096 lr:8.0e-07 updt_s:4.673 data_s:0.001\n",
      "WARNING 2025-08-11 14:43:27 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:43:27 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:43:32 in_model.py:238 step:8 smpl:3K ep:7 epch:0.12 loss:1.567 grdn:6.004 lr:9.0e-07 updt_s:4.746 data_s:0.001\n",
      "WARNING 2025-08-11 14:43:32 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:43:32 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:43:36 in_model.py:238 step:9 smpl:3K ep:7 epch:0.14 loss:1.522 grdn:6.095 lr:1.0e-06 updt_s:4.694 data_s:0.001\n",
      "WARNING 2025-08-11 14:43:36 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:43:36 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:43:41 in_model.py:238 step:10 smpl:4K ep:8 epch:0.15 loss:1.500 grdn:5.978 lr:1.1e-06 updt_s:4.686 data_s:0.001\n",
      "WARNING 2025-08-11 14:43:41 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:43:41 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 10 of 1000\n",
      "INFO 2025-08-11 14:43:46 in_model.py:238 step:11 smpl:4K ep:9 epch:0.17 loss:1.527 grdn:6.070 lr:1.2e-06 updt_s:4.798 data_s:0.001\n",
      "WARNING 2025-08-11 14:43:46 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:43:46 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:43:50 in_model.py:238 step:12 smpl:4K ep:10 epch:0.18 loss:1.550 grdn:6.010 lr:1.3e-06 updt_s:4.709 data_s:0.001\n",
      "WARNING 2025-08-11 14:43:50 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:43:50 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:43:55 in_model.py:238 step:13 smpl:5K ep:11 epch:0.20 loss:1.510 grdn:6.008 lr:1.4e-06 updt_s:4.827 data_s:0.001\n",
      "WARNING 2025-08-11 14:43:55 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:43:55 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:44:00 in_model.py:238 step:14 smpl:5K ep:12 epch:0.21 loss:1.518 grdn:5.900 lr:1.5e-06 updt_s:4.759 data_s:0.001\n",
      "WARNING 2025-08-11 14:44:00 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:44:00 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:44:05 in_model.py:238 step:15 smpl:5K ep:12 epch:0.23 loss:1.521 grdn:6.002 lr:1.6e-06 updt_s:4.809 data_s:0.000\n",
      "WARNING 2025-08-11 14:44:05 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:44:05 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:44:10 in_model.py:238 step:16 smpl:6K ep:13 epch:0.24 loss:1.486 grdn:6.125 lr:1.7e-06 updt_s:4.722 data_s:0.001\n",
      "WARNING 2025-08-11 14:44:10 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:44:10 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:44:14 in_model.py:238 step:17 smpl:6K ep:14 epch:0.26 loss:1.503 grdn:5.834 lr:1.8e-06 updt_s:4.689 data_s:0.001\n",
      "WARNING 2025-08-11 14:44:14 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:44:14 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:44:19 in_model.py:238 step:18 smpl:6K ep:15 epch:0.27 loss:1.518 grdn:5.664 lr:1.9e-06 updt_s:4.704 data_s:0.001\n",
      "WARNING 2025-08-11 14:44:19 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:44:19 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:44:24 in_model.py:238 step:19 smpl:7K ep:16 epch:0.29 loss:1.511 grdn:5.805 lr:2.0e-06 updt_s:4.727 data_s:0.003\n",
      "WARNING 2025-08-11 14:44:24 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:44:24 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:44:28 in_model.py:238 step:20 smpl:7K ep:17 epch:0.30 loss:1.514 grdn:5.506 lr:2.1e-06 updt_s:4.703 data_s:0.000\n",
      "WARNING 2025-08-11 14:44:28 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:44:28 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 20 of 1000\n",
      "INFO 2025-08-11 14:44:33 in_model.py:238 step:21 smpl:7K ep:17 epch:0.32 loss:1.462 grdn:5.707 lr:2.2e-06 updt_s:4.676 data_s:0.000\n",
      "WARNING 2025-08-11 14:44:33 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:44:33 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:44:38 in_model.py:238 step:22 smpl:8K ep:18 epch:0.33 loss:1.467 grdn:5.530 lr:2.3e-06 updt_s:4.680 data_s:0.000\n",
      "WARNING 2025-08-11 14:44:38 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:44:38 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:44:42 in_model.py:238 step:23 smpl:8K ep:19 epch:0.35 loss:1.481 grdn:5.410 lr:2.4e-06 updt_s:4.684 data_s:0.000\n",
      "WARNING 2025-08-11 14:44:42 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:44:43 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:44:47 in_model.py:238 step:24 smpl:8K ep:20 epch:0.36 loss:1.536 grdn:5.396 lr:2.5e-06 updt_s:4.691 data_s:0.000\n",
      "WARNING 2025-08-11 14:44:47 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:44:47 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:44:52 in_model.py:238 step:25 smpl:9K ep:21 epch:0.38 loss:1.472 grdn:5.237 lr:2.6e-06 updt_s:4.675 data_s:0.001\n",
      "WARNING 2025-08-11 14:44:52 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:44:52 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:44:57 in_model.py:238 step:26 smpl:9K ep:22 epch:0.39 loss:1.436 grdn:5.213 lr:2.7e-06 updt_s:4.665 data_s:0.000\n",
      "WARNING 2025-08-11 14:44:57 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:44:57 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:45:01 in_model.py:238 step:27 smpl:9K ep:22 epch:0.41 loss:1.463 grdn:5.072 lr:2.8e-06 updt_s:4.681 data_s:0.000\n",
      "WARNING 2025-08-11 14:45:01 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:45:01 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:45:06 in_model.py:238 step:28 smpl:10K ep:23 epch:0.42 loss:1.413 grdn:5.153 lr:2.9e-06 updt_s:4.698 data_s:0.000\n",
      "WARNING 2025-08-11 14:45:06 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:45:06 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:45:11 in_model.py:238 step:29 smpl:10K ep:24 epch:0.44 loss:1.425 grdn:4.958 lr:3.0e-06 updt_s:4.685 data_s:0.000\n",
      "WARNING 2025-08-11 14:45:11 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:45:11 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:45:15 in_model.py:238 step:30 smpl:10K ep:25 epch:0.45 loss:1.444 grdn:5.101 lr:3.1e-06 updt_s:4.689 data_s:0.000\n",
      "WARNING 2025-08-11 14:45:15 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:45:15 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 30 of 1000\n",
      "INFO 2025-08-11 14:45:20 in_model.py:238 step:31 smpl:11K ep:26 epch:0.47 loss:1.431 grdn:4.944 lr:3.2e-06 updt_s:4.689 data_s:0.000\n",
      "WARNING 2025-08-11 14:45:20 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:45:20 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:45:25 in_model.py:238 step:32 smpl:11K ep:27 epch:0.48 loss:1.407 grdn:4.670 lr:3.3e-06 updt_s:4.684 data_s:0.000\n",
      "WARNING 2025-08-11 14:45:25 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:45:25 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:45:29 in_model.py:238 step:33 smpl:12K ep:27 epch:0.50 loss:1.474 grdn:4.792 lr:3.4e-06 updt_s:4.700 data_s:0.000\n",
      "WARNING 2025-08-11 14:45:29 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:45:29 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:45:34 in_model.py:238 step:34 smpl:12K ep:28 epch:0.51 loss:1.412 grdn:4.526 lr:3.5e-06 updt_s:4.685 data_s:0.000\n",
      "WARNING 2025-08-11 14:45:34 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:45:34 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:45:39 in_model.py:238 step:35 smpl:12K ep:29 epch:0.53 loss:1.442 grdn:4.487 lr:3.6e-06 updt_s:4.689 data_s:0.000\n",
      "WARNING 2025-08-11 14:45:39 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:45:39 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:45:43 in_model.py:238 step:36 smpl:13K ep:30 epch:0.54 loss:1.411 grdn:4.402 lr:3.7e-06 updt_s:4.702 data_s:0.000\n",
      "WARNING 2025-08-11 14:45:43 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:45:43 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:45:48 in_model.py:238 step:37 smpl:13K ep:31 epch:0.56 loss:1.416 grdn:4.324 lr:3.8e-06 updt_s:4.689 data_s:0.000\n",
      "WARNING 2025-08-11 14:45:48 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:45:48 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:45:53 in_model.py:238 step:38 smpl:13K ep:32 epch:0.57 loss:1.395 grdn:4.137 lr:3.9e-06 updt_s:4.686 data_s:0.000\n",
      "WARNING 2025-08-11 14:45:53 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:45:53 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:45:58 in_model.py:238 step:39 smpl:14K ep:32 epch:0.59 loss:1.393 grdn:4.144 lr:4.0e-06 updt_s:4.690 data_s:0.000\n",
      "WARNING 2025-08-11 14:45:58 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:45:58 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:46:02 in_model.py:238 step:40 smpl:14K ep:33 epch:0.60 loss:1.377 grdn:4.010 lr:4.1e-06 updt_s:4.687 data_s:0.000\n",
      "WARNING 2025-08-11 14:46:02 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:46:02 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 40 of 1000\n",
      "INFO 2025-08-11 14:46:07 in_model.py:238 step:41 smpl:14K ep:34 epch:0.62 loss:1.401 grdn:3.928 lr:4.2e-06 updt_s:4.686 data_s:0.000\n",
      "WARNING 2025-08-11 14:46:07 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:46:07 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:46:12 in_model.py:238 step:42 smpl:15K ep:35 epch:0.63 loss:1.370 grdn:3.786 lr:4.3e-06 updt_s:4.699 data_s:0.000\n",
      "WARNING 2025-08-11 14:46:12 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:46:12 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:46:16 in_model.py:238 step:43 smpl:15K ep:36 epch:0.65 loss:1.354 grdn:3.610 lr:4.4e-06 updt_s:4.690 data_s:0.000\n",
      "WARNING 2025-08-11 14:46:16 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:46:16 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:46:21 in_model.py:238 step:44 smpl:15K ep:37 epch:0.66 loss:1.331 grdn:3.549 lr:4.5e-06 updt_s:4.690 data_s:0.000\n",
      "WARNING 2025-08-11 14:46:21 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:46:21 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:46:26 in_model.py:238 step:45 smpl:16K ep:37 epch:0.68 loss:1.313 grdn:3.374 lr:4.6e-06 updt_s:4.701 data_s:0.000\n",
      "WARNING 2025-08-11 14:46:26 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:46:26 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:46:30 in_model.py:238 step:46 smpl:16K ep:38 epch:0.69 loss:1.367 grdn:3.211 lr:4.7e-06 updt_s:4.693 data_s:0.000\n",
      "WARNING 2025-08-11 14:46:30 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:46:30 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:46:35 in_model.py:238 step:47 smpl:16K ep:39 epch:0.71 loss:1.294 grdn:3.009 lr:4.8e-06 updt_s:4.687 data_s:0.000\n",
      "WARNING 2025-08-11 14:46:35 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:46:35 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:46:40 in_model.py:238 step:48 smpl:17K ep:40 epch:0.72 loss:1.325 grdn:2.741 lr:4.9e-06 updt_s:4.698 data_s:0.000\n",
      "WARNING 2025-08-11 14:46:40 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:46:40 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:46:45 in_model.py:238 step:49 smpl:17K ep:41 epch:0.74 loss:1.299 grdn:2.726 lr:5.0e-06 updt_s:4.693 data_s:0.000\n",
      "WARNING 2025-08-11 14:46:45 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:46:45 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:46:49 in_model.py:238 step:50 smpl:18K ep:42 epch:0.75 loss:1.294 grdn:2.405 lr:5.1e-06 updt_s:4.694 data_s:0.000\n",
      "WARNING 2025-08-11 14:46:49 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:46:49 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:46:49 in_model.py:247 Checkpoint policy after step 50\n",
      "Step 50 of 1000\n",
      "INFO 2025-08-11 14:46:57 in_model.py:238 step:51 smpl:18K ep:42 epch:0.77 loss:1.322 grdn:2.464 lr:5.2e-06 updt_s:4.684 data_s:0.001\n",
      "WARNING 2025-08-11 14:46:57 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:46:57 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:47:02 in_model.py:238 step:52 smpl:18K ep:43 epch:0.78 loss:1.302 grdn:2.336 lr:5.3e-06 updt_s:4.669 data_s:0.000\n",
      "WARNING 2025-08-11 14:47:02 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:47:02 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:47:06 in_model.py:238 step:53 smpl:19K ep:44 epch:0.80 loss:1.269 grdn:2.027 lr:5.4e-06 updt_s:4.696 data_s:0.000\n",
      "WARNING 2025-08-11 14:47:06 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:47:06 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:47:11 in_model.py:238 step:54 smpl:19K ep:45 epch:0.82 loss:1.259 grdn:2.034 lr:5.5e-06 updt_s:4.691 data_s:0.000\n",
      "WARNING 2025-08-11 14:47:11 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:47:11 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:47:16 in_model.py:238 step:55 smpl:19K ep:46 epch:0.83 loss:1.265 grdn:1.793 lr:5.6e-06 updt_s:4.700 data_s:0.000\n",
      "WARNING 2025-08-11 14:47:16 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:47:16 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:47:20 in_model.py:238 step:56 smpl:20K ep:46 epch:0.85 loss:1.258 grdn:1.676 lr:5.7e-06 updt_s:4.699 data_s:0.000\n",
      "WARNING 2025-08-11 14:47:20 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:47:20 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:47:25 in_model.py:238 step:57 smpl:20K ep:47 epch:0.86 loss:1.268 grdn:1.622 lr:5.8e-06 updt_s:4.684 data_s:0.000\n",
      "WARNING 2025-08-11 14:47:25 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:47:25 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:47:30 in_model.py:238 step:58 smpl:20K ep:48 epch:0.88 loss:1.280 grdn:1.754 lr:5.9e-06 updt_s:4.699 data_s:0.000\n",
      "WARNING 2025-08-11 14:47:30 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:47:30 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:47:34 in_model.py:238 step:59 smpl:21K ep:49 epch:0.89 loss:1.278 grdn:1.433 lr:6.0e-06 updt_s:4.697 data_s:0.000\n",
      "WARNING 2025-08-11 14:47:34 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:47:34 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:47:39 in_model.py:238 step:60 smpl:21K ep:50 epch:0.91 loss:1.272 grdn:1.260 lr:6.1e-06 updt_s:4.693 data_s:0.000\n",
      "WARNING 2025-08-11 14:47:39 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:47:39 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 60 of 1000\n",
      "INFO 2025-08-11 14:47:44 in_model.py:238 step:61 smpl:21K ep:51 epch:0.92 loss:1.275 grdn:1.260 lr:6.2e-06 updt_s:4.697 data_s:0.000\n",
      "WARNING 2025-08-11 14:47:44 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:47:44 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:47:49 in_model.py:238 step:62 smpl:22K ep:51 epch:0.94 loss:1.291 grdn:1.294 lr:6.3e-06 updt_s:4.693 data_s:0.000\n",
      "WARNING 2025-08-11 14:47:49 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:47:49 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:47:53 in_model.py:238 step:63 smpl:22K ep:52 epch:0.95 loss:1.220 grdn:1.271 lr:6.4e-06 updt_s:4.690 data_s:0.000\n",
      "WARNING 2025-08-11 14:47:53 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:47:53 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:47:58 in_model.py:238 step:64 smpl:22K ep:53 epch:0.97 loss:1.266 grdn:1.076 lr:6.5e-06 updt_s:4.695 data_s:0.000\n",
      "WARNING 2025-08-11 14:47:58 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:47:58 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:48:03 in_model.py:238 step:65 smpl:23K ep:54 epch:0.98 loss:1.278 grdn:1.162 lr:6.6e-06 updt_s:4.682 data_s:0.000\n",
      "WARNING 2025-08-11 14:48:03 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:48:03 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:48:07 in_model.py:238 step:66 smpl:23K ep:55 epch:1.00 loss:1.256 grdn:1.104 lr:6.7e-06 updt_s:4.697 data_s:0.000\n",
      "WARNING 2025-08-11 14:48:07 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:48:07 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:48:09 in_model.py:238 step:67 smpl:23K ep:56 epch:1.01 loss:1.253 grdn:1.799 lr:6.8e-06 updt_s:1.503 data_s:0.000\n",
      "WARNING 2025-08-11 14:48:09 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:48:09 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-08-11 14:48:29 in_model.py:238 step:68 smpl:24K ep:56 epch:1.03 loss:1.218 grdn:1.060 lr:6.9e-06 updt_s:4.877 data_s:14.809\n",
      "WARNING 2025-08-11 14:48:29 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:48:29 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:48:33 in_model.py:238 step:69 smpl:24K ep:57 epch:1.04 loss:1.246 grdn:1.109 lr:7.0e-06 updt_s:4.788 data_s:0.001\n",
      "WARNING 2025-08-11 14:48:33 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:48:33 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:48:38 in_model.py:238 step:70 smpl:24K ep:58 epch:1.06 loss:1.247 grdn:1.057 lr:7.1e-06 updt_s:4.690 data_s:0.001\n",
      "WARNING 2025-08-11 14:48:38 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:48:38 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 70 of 1000\n",
      "INFO 2025-08-11 14:48:43 in_model.py:238 step:71 smpl:25K ep:59 epch:1.07 loss:1.250 grdn:1.018 lr:7.2e-06 updt_s:4.706 data_s:0.000\n",
      "WARNING 2025-08-11 14:48:43 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:48:43 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:48:47 in_model.py:238 step:72 smpl:25K ep:60 epch:1.09 loss:1.227 grdn:1.059 lr:7.3e-06 updt_s:4.696 data_s:0.001\n",
      "WARNING 2025-08-11 14:48:47 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:48:47 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:48:52 in_model.py:238 step:73 smpl:26K ep:61 epch:1.10 loss:1.217 grdn:1.057 lr:7.4e-06 updt_s:4.725 data_s:0.001\n",
      "WARNING 2025-08-11 14:48:52 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:48:52 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:48:57 in_model.py:238 step:74 smpl:26K ep:61 epch:1.12 loss:1.230 grdn:1.059 lr:7.5e-06 updt_s:4.708 data_s:0.000\n",
      "WARNING 2025-08-11 14:48:57 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:48:57 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:49:02 in_model.py:238 step:75 smpl:26K ep:62 epch:1.13 loss:1.229 grdn:1.040 lr:7.6e-06 updt_s:4.720 data_s:0.001\n",
      "WARNING 2025-08-11 14:49:02 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:49:02 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:49:06 in_model.py:238 step:76 smpl:27K ep:63 epch:1.15 loss:1.226 grdn:1.016 lr:7.7e-06 updt_s:4.726 data_s:0.001\n",
      "WARNING 2025-08-11 14:49:06 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:49:06 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:49:11 in_model.py:238 step:77 smpl:27K ep:64 epch:1.16 loss:1.232 grdn:0.955 lr:7.8e-06 updt_s:4.715 data_s:0.001\n",
      "WARNING 2025-08-11 14:49:11 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:49:11 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:49:16 in_model.py:238 step:78 smpl:27K ep:65 epch:1.18 loss:1.267 grdn:1.089 lr:7.9e-06 updt_s:4.719 data_s:0.001\n",
      "WARNING 2025-08-11 14:49:16 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:49:16 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:49:21 in_model.py:238 step:79 smpl:28K ep:66 epch:1.19 loss:1.231 grdn:1.055 lr:8.0e-06 updt_s:4.712 data_s:0.001\n",
      "WARNING 2025-08-11 14:49:21 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:49:21 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:49:25 in_model.py:238 step:80 smpl:28K ep:66 epch:1.21 loss:1.243 grdn:1.045 lr:8.1e-06 updt_s:4.718 data_s:0.000\n",
      "WARNING 2025-08-11 14:49:25 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:49:25 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 80 of 1000\n",
      "INFO 2025-08-11 14:49:30 in_model.py:238 step:81 smpl:28K ep:67 epch:1.22 loss:1.214 grdn:1.092 lr:8.2e-06 updt_s:4.712 data_s:0.001\n",
      "WARNING 2025-08-11 14:49:30 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:49:30 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:49:35 in_model.py:238 step:82 smpl:29K ep:68 epch:1.24 loss:1.212 grdn:0.886 lr:8.3e-06 updt_s:4.765 data_s:0.001\n",
      "WARNING 2025-08-11 14:49:35 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:49:35 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:49:40 in_model.py:238 step:83 smpl:29K ep:69 epch:1.25 loss:1.200 grdn:0.914 lr:8.4e-06 updt_s:4.814 data_s:0.000\n",
      "WARNING 2025-08-11 14:49:40 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:49:40 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:49:44 in_model.py:238 step:84 smpl:29K ep:70 epch:1.27 loss:1.218 grdn:0.972 lr:8.5e-06 updt_s:4.714 data_s:0.000\n",
      "WARNING 2025-08-11 14:49:44 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:49:44 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:49:49 in_model.py:238 step:85 smpl:30K ep:71 epch:1.28 loss:1.195 grdn:0.937 lr:8.6e-06 updt_s:4.725 data_s:0.000\n",
      "WARNING 2025-08-11 14:49:49 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:49:49 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:49:54 in_model.py:238 step:86 smpl:30K ep:71 epch:1.30 loss:1.219 grdn:1.099 lr:8.7e-06 updt_s:4.713 data_s:0.002\n",
      "WARNING 2025-08-11 14:49:54 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:49:54 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:49:58 in_model.py:238 step:87 smpl:30K ep:72 epch:1.31 loss:1.217 grdn:0.938 lr:8.8e-06 updt_s:4.696 data_s:0.000\n",
      "WARNING 2025-08-11 14:49:58 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:49:58 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:50:03 in_model.py:238 step:88 smpl:31K ep:73 epch:1.33 loss:1.204 grdn:0.933 lr:8.9e-06 updt_s:4.702 data_s:0.000\n",
      "WARNING 2025-08-11 14:50:03 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:50:03 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:50:08 in_model.py:238 step:89 smpl:31K ep:74 epch:1.34 loss:1.215 grdn:0.981 lr:9.0e-06 updt_s:4.697 data_s:0.000\n",
      "WARNING 2025-08-11 14:50:08 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:50:08 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:50:13 in_model.py:238 step:90 smpl:32K ep:75 epch:1.36 loss:1.194 grdn:0.898 lr:9.1e-06 updt_s:4.693 data_s:0.000\n",
      "WARNING 2025-08-11 14:50:13 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:50:13 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 90 of 1000\n",
      "INFO 2025-08-11 14:50:17 in_model.py:238 step:91 smpl:32K ep:76 epch:1.37 loss:1.196 grdn:0.952 lr:9.2e-06 updt_s:4.694 data_s:0.000\n",
      "WARNING 2025-08-11 14:50:17 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:50:17 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:50:22 in_model.py:238 step:92 smpl:32K ep:76 epch:1.39 loss:1.190 grdn:0.854 lr:9.3e-06 updt_s:4.687 data_s:0.000\n",
      "WARNING 2025-08-11 14:50:22 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:50:22 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:50:27 in_model.py:238 step:93 smpl:33K ep:77 epch:1.40 loss:1.244 grdn:1.116 lr:9.4e-06 updt_s:4.691 data_s:0.000\n",
      "WARNING 2025-08-11 14:50:27 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:50:27 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:50:31 in_model.py:238 step:94 smpl:33K ep:78 epch:1.42 loss:1.186 grdn:0.870 lr:9.5e-06 updt_s:4.693 data_s:0.000\n",
      "WARNING 2025-08-11 14:50:31 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:50:31 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:50:36 in_model.py:238 step:95 smpl:33K ep:79 epch:1.43 loss:1.227 grdn:0.924 lr:9.6e-06 updt_s:4.698 data_s:0.000\n",
      "WARNING 2025-08-11 14:50:36 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:50:36 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:50:41 in_model.py:238 step:96 smpl:34K ep:80 epch:1.45 loss:1.232 grdn:0.914 lr:9.7e-06 updt_s:4.693 data_s:0.000\n",
      "WARNING 2025-08-11 14:50:41 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:50:41 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:50:45 in_model.py:238 step:97 smpl:34K ep:81 epch:1.46 loss:1.196 grdn:0.980 lr:9.8e-06 updt_s:4.696 data_s:0.000\n",
      "WARNING 2025-08-11 14:50:45 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:50:45 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:50:50 in_model.py:238 step:98 smpl:34K ep:81 epch:1.48 loss:1.170 grdn:0.934 lr:9.9e-06 updt_s:4.691 data_s:0.000\n",
      "WARNING 2025-08-11 14:50:50 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:50:50 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:50:55 in_model.py:238 step:99 smpl:35K ep:82 epch:1.49 loss:1.185 grdn:0.930 lr:1.0e-05 updt_s:4.686 data_s:0.000\n",
      "WARNING 2025-08-11 14:50:55 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:50:55 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:50:59 in_model.py:238 step:100 smpl:35K ep:83 epch:1.51 loss:1.167 grdn:0.933 lr:1.0e-05 updt_s:4.688 data_s:0.000\n",
      "WARNING 2025-08-11 14:50:59 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:50:59 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:50:59 in_model.py:247 Checkpoint policy after step 100\n",
      "Step 100 of 1000\n",
      "INFO 2025-08-11 14:51:07 in_model.py:238 step:101 smpl:35K ep:84 epch:1.52 loss:1.163 grdn:0.877 lr:1.0e-05 updt_s:4.674 data_s:0.000\n",
      "WARNING 2025-08-11 14:51:07 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:51:07 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:51:12 in_model.py:238 step:102 smpl:36K ep:85 epch:1.54 loss:1.186 grdn:0.809 lr:1.0e-05 updt_s:4.688 data_s:0.000\n",
      "WARNING 2025-08-11 14:51:12 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:51:12 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:51:17 in_model.py:238 step:103 smpl:36K ep:86 epch:1.55 loss:1.133 grdn:0.869 lr:1.0e-05 updt_s:4.702 data_s:0.000\n",
      "WARNING 2025-08-11 14:51:17 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:51:17 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:51:21 in_model.py:238 step:104 smpl:36K ep:86 epch:1.57 loss:1.182 grdn:0.885 lr:1.0e-05 updt_s:4.701 data_s:0.000\n",
      "WARNING 2025-08-11 14:51:21 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:51:21 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:51:26 in_model.py:238 step:105 smpl:37K ep:87 epch:1.58 loss:1.189 grdn:0.828 lr:1.1e-05 updt_s:4.702 data_s:0.000\n",
      "WARNING 2025-08-11 14:51:26 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:51:26 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:51:31 in_model.py:238 step:106 smpl:37K ep:88 epch:1.60 loss:1.146 grdn:0.829 lr:1.1e-05 updt_s:4.701 data_s:0.000\n",
      "WARNING 2025-08-11 14:51:31 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:51:31 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:51:35 in_model.py:238 step:107 smpl:37K ep:89 epch:1.61 loss:1.175 grdn:0.846 lr:1.1e-05 updt_s:4.698 data_s:0.000\n",
      "WARNING 2025-08-11 14:51:35 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:51:35 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:51:40 in_model.py:238 step:108 smpl:38K ep:90 epch:1.63 loss:1.159 grdn:0.890 lr:1.1e-05 updt_s:4.703 data_s:0.000\n",
      "WARNING 2025-08-11 14:51:40 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:51:40 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:51:45 in_model.py:238 step:109 smpl:38K ep:90 epch:1.65 loss:1.169 grdn:0.991 lr:1.1e-05 updt_s:4.695 data_s:0.000\n",
      "WARNING 2025-08-11 14:51:45 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:51:45 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:51:49 in_model.py:238 step:110 smpl:38K ep:91 epch:1.66 loss:1.148 grdn:0.941 lr:1.1e-05 updt_s:4.692 data_s:0.000\n",
      "WARNING 2025-08-11 14:51:49 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:51:49 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 110 of 1000\n",
      "INFO 2025-08-11 14:51:54 in_model.py:238 step:111 smpl:39K ep:92 epch:1.68 loss:1.156 grdn:0.981 lr:1.1e-05 updt_s:4.699 data_s:0.000\n",
      "WARNING 2025-08-11 14:51:54 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:51:54 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:51:59 in_model.py:238 step:112 smpl:39K ep:93 epch:1.69 loss:1.161 grdn:0.807 lr:1.1e-05 updt_s:4.714 data_s:0.000\n",
      "WARNING 2025-08-11 14:51:59 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:51:59 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:52:04 in_model.py:238 step:113 smpl:40K ep:94 epch:1.71 loss:1.165 grdn:0.977 lr:1.1e-05 updt_s:4.692 data_s:0.000\n",
      "WARNING 2025-08-11 14:52:04 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:52:04 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:52:08 in_model.py:238 step:114 smpl:40K ep:95 epch:1.72 loss:1.161 grdn:0.778 lr:1.1e-05 updt_s:4.694 data_s:0.000\n",
      "WARNING 2025-08-11 14:52:08 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:52:08 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:52:13 in_model.py:238 step:115 smpl:40K ep:95 epch:1.74 loss:1.176 grdn:0.916 lr:1.2e-05 updt_s:4.717 data_s:0.000\n",
      "WARNING 2025-08-11 14:52:13 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:52:13 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:52:18 in_model.py:238 step:116 smpl:41K ep:96 epch:1.75 loss:1.150 grdn:0.814 lr:1.2e-05 updt_s:4.697 data_s:0.000\n",
      "WARNING 2025-08-11 14:52:18 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:52:18 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:52:22 in_model.py:238 step:117 smpl:41K ep:97 epch:1.77 loss:1.158 grdn:1.055 lr:1.2e-05 updt_s:4.706 data_s:0.000\n",
      "WARNING 2025-08-11 14:52:22 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:52:22 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:52:27 in_model.py:238 step:118 smpl:41K ep:98 epch:1.78 loss:1.185 grdn:0.820 lr:1.2e-05 updt_s:4.698 data_s:0.000\n",
      "WARNING 2025-08-11 14:52:27 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:52:27 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:52:32 in_model.py:238 step:119 smpl:42K ep:99 epch:1.80 loss:1.144 grdn:0.809 lr:1.2e-05 updt_s:4.713 data_s:0.000\n",
      "WARNING 2025-08-11 14:52:32 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:52:32 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:52:37 in_model.py:238 step:120 smpl:42K ep:100 epch:1.81 loss:1.113 grdn:0.991 lr:1.2e-05 updt_s:4.707 data_s:0.001\n",
      "WARNING 2025-08-11 14:52:37 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:52:37 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 120 of 1000\n",
      "INFO 2025-08-11 14:52:41 in_model.py:238 step:121 smpl:42K ep:100 epch:1.83 loss:1.149 grdn:0.912 lr:1.2e-05 updt_s:4.701 data_s:0.000\n",
      "WARNING 2025-08-11 14:52:41 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:52:41 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:52:46 in_model.py:238 step:122 smpl:43K ep:101 epch:1.84 loss:1.135 grdn:0.889 lr:1.2e-05 updt_s:4.701 data_s:0.000\n",
      "WARNING 2025-08-11 14:52:46 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:52:46 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:52:51 in_model.py:238 step:123 smpl:43K ep:102 epch:1.86 loss:1.132 grdn:0.908 lr:1.2e-05 updt_s:4.690 data_s:0.000\n",
      "WARNING 2025-08-11 14:52:51 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:52:51 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:52:55 in_model.py:238 step:124 smpl:43K ep:103 epch:1.87 loss:1.137 grdn:0.912 lr:1.2e-05 updt_s:4.687 data_s:0.000\n",
      "WARNING 2025-08-11 14:52:55 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:52:55 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:53:00 in_model.py:238 step:125 smpl:44K ep:104 epch:1.89 loss:1.129 grdn:0.859 lr:1.3e-05 updt_s:4.698 data_s:0.000\n",
      "WARNING 2025-08-11 14:53:00 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:53:00 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:53:05 in_model.py:238 step:126 smpl:44K ep:105 epch:1.90 loss:1.114 grdn:0.862 lr:1.3e-05 updt_s:4.698 data_s:0.000\n",
      "WARNING 2025-08-11 14:53:05 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:53:05 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:53:09 in_model.py:238 step:127 smpl:44K ep:105 epch:1.92 loss:1.137 grdn:0.925 lr:1.3e-05 updt_s:4.699 data_s:0.000\n",
      "WARNING 2025-08-11 14:53:09 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:53:09 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:53:14 in_model.py:238 step:128 smpl:45K ep:106 epch:1.93 loss:1.130 grdn:0.870 lr:1.3e-05 updt_s:4.697 data_s:0.000\n",
      "WARNING 2025-08-11 14:53:14 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:53:14 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:53:19 in_model.py:238 step:129 smpl:45K ep:107 epch:1.95 loss:1.106 grdn:1.037 lr:1.3e-05 updt_s:4.696 data_s:0.000\n",
      "WARNING 2025-08-11 14:53:19 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:53:19 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:53:24 in_model.py:238 step:130 smpl:46K ep:108 epch:1.96 loss:1.131 grdn:0.846 lr:1.3e-05 updt_s:4.693 data_s:0.000\n",
      "WARNING 2025-08-11 14:53:24 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:53:24 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 130 of 1000\n",
      "INFO 2025-08-11 14:53:28 in_model.py:238 step:131 smpl:46K ep:109 epch:1.98 loss:1.082 grdn:0.873 lr:1.3e-05 updt_s:4.694 data_s:0.000\n",
      "WARNING 2025-08-11 14:53:28 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:53:28 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:53:33 in_model.py:238 step:132 smpl:46K ep:110 epch:1.99 loss:1.118 grdn:0.821 lr:1.3e-05 updt_s:4.713 data_s:0.000\n",
      "WARNING 2025-08-11 14:53:33 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:53:33 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:53:38 in_model.py:238 step:133 smpl:47K ep:110 epch:2.01 loss:1.098 grdn:0.939 lr:1.3e-05 updt_s:4.719 data_s:0.000\n",
      "WARNING 2025-08-11 14:53:38 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:53:38 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:53:39 in_model.py:238 step:134 smpl:47K ep:111 epch:2.02 loss:1.107 grdn:1.604 lr:1.3e-05 updt_s:1.229 data_s:0.000\n",
      "WARNING 2025-08-11 14:53:39 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:53:39 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-08-11 14:53:59 in_model.py:238 step:135 smpl:47K ep:112 epch:2.04 loss:1.109 grdn:0.887 lr:1.4e-05 updt_s:4.855 data_s:14.789\n",
      "WARNING 2025-08-11 14:53:59 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:53:59 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:54:03 in_model.py:238 step:136 smpl:48K ep:113 epch:2.05 loss:1.105 grdn:0.863 lr:1.4e-05 updt_s:4.783 data_s:0.001\n",
      "WARNING 2025-08-11 14:54:03 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:54:03 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:54:08 in_model.py:238 step:137 smpl:48K ep:114 epch:2.07 loss:1.105 grdn:0.976 lr:1.4e-05 updt_s:4.808 data_s:0.000\n",
      "WARNING 2025-08-11 14:54:08 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:54:08 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:54:13 in_model.py:238 step:138 smpl:48K ep:115 epch:2.08 loss:1.071 grdn:0.908 lr:1.4e-05 updt_s:4.700 data_s:0.001\n",
      "WARNING 2025-08-11 14:54:13 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:54:13 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:54:18 in_model.py:238 step:139 smpl:49K ep:115 epch:2.10 loss:1.127 grdn:0.979 lr:1.4e-05 updt_s:4.707 data_s:0.000\n",
      "WARNING 2025-08-11 14:54:18 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:54:18 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:54:22 in_model.py:238 step:140 smpl:49K ep:116 epch:2.11 loss:1.101 grdn:0.989 lr:1.4e-05 updt_s:4.715 data_s:0.001\n",
      "WARNING 2025-08-11 14:54:22 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:54:22 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 140 of 1000\n",
      "INFO 2025-08-11 14:54:27 in_model.py:238 step:141 smpl:49K ep:117 epch:2.13 loss:1.099 grdn:0.913 lr:1.4e-05 updt_s:4.708 data_s:0.000\n",
      "WARNING 2025-08-11 14:54:27 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:54:27 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:54:32 in_model.py:238 step:142 smpl:50K ep:118 epch:2.14 loss:1.052 grdn:0.987 lr:1.4e-05 updt_s:4.717 data_s:0.000\n",
      "WARNING 2025-08-11 14:54:32 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:54:32 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:54:36 in_model.py:238 step:143 smpl:50K ep:119 epch:2.16 loss:1.053 grdn:1.056 lr:1.4e-05 updt_s:4.724 data_s:0.000\n",
      "WARNING 2025-08-11 14:54:36 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:54:36 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:54:41 in_model.py:238 step:144 smpl:50K ep:120 epch:2.17 loss:1.086 grdn:1.049 lr:1.4e-05 updt_s:4.719 data_s:0.000\n",
      "WARNING 2025-08-11 14:54:41 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:54:41 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:54:46 in_model.py:238 step:145 smpl:51K ep:120 epch:2.19 loss:1.092 grdn:1.013 lr:1.5e-05 updt_s:4.719 data_s:0.001\n",
      "WARNING 2025-08-11 14:54:46 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:54:46 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:54:51 in_model.py:238 step:146 smpl:51K ep:121 epch:2.20 loss:1.078 grdn:0.972 lr:1.5e-05 updt_s:4.746 data_s:0.000\n",
      "WARNING 2025-08-11 14:54:51 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:54:51 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:54:55 in_model.py:238 step:147 smpl:51K ep:122 epch:2.22 loss:1.032 grdn:0.999 lr:1.5e-05 updt_s:4.717 data_s:0.000\n",
      "WARNING 2025-08-11 14:54:55 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:54:55 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:55:00 in_model.py:238 step:148 smpl:52K ep:123 epch:2.23 loss:1.043 grdn:0.968 lr:1.5e-05 updt_s:4.736 data_s:0.001\n",
      "WARNING 2025-08-11 14:55:00 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:55:00 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:55:05 in_model.py:238 step:149 smpl:52K ep:124 epch:2.25 loss:1.042 grdn:1.061 lr:1.5e-05 updt_s:4.731 data_s:0.001\n",
      "WARNING 2025-08-11 14:55:05 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:55:05 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:55:10 in_model.py:238 step:150 smpl:52K ep:125 epch:2.26 loss:1.041 grdn:1.045 lr:1.5e-05 updt_s:4.725 data_s:0.001\n",
      "WARNING 2025-08-11 14:55:10 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:55:10 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:55:10 in_model.py:247 Checkpoint policy after step 150\n",
      "Step 150 of 1000\n",
      "INFO 2025-08-11 14:55:19 in_model.py:238 step:151 smpl:53K ep:125 epch:2.28 loss:1.060 grdn:1.132 lr:1.5e-05 updt_s:4.708 data_s:0.001\n",
      "WARNING 2025-08-11 14:55:19 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:55:19 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:55:24 in_model.py:238 step:152 smpl:53K ep:126 epch:2.29 loss:1.022 grdn:1.076 lr:1.5e-05 updt_s:4.719 data_s:0.000\n",
      "WARNING 2025-08-11 14:55:24 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:55:24 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:55:29 in_model.py:238 step:153 smpl:54K ep:127 epch:2.31 loss:1.004 grdn:1.000 lr:1.5e-05 updt_s:4.713 data_s:0.004\n",
      "WARNING 2025-08-11 14:55:29 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:55:29 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:55:33 in_model.py:238 step:154 smpl:54K ep:128 epch:2.32 loss:0.999 grdn:1.037 lr:1.5e-05 updt_s:4.697 data_s:0.000\n",
      "WARNING 2025-08-11 14:55:33 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:55:33 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:55:38 in_model.py:238 step:155 smpl:54K ep:129 epch:2.34 loss:1.003 grdn:1.115 lr:1.6e-05 updt_s:4.700 data_s:0.000\n",
      "WARNING 2025-08-11 14:55:38 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:55:38 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:55:43 in_model.py:238 step:156 smpl:55K ep:129 epch:2.35 loss:0.987 grdn:1.054 lr:1.6e-05 updt_s:4.698 data_s:0.000\n",
      "WARNING 2025-08-11 14:55:43 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:55:43 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:55:47 in_model.py:238 step:157 smpl:55K ep:130 epch:2.37 loss:1.001 grdn:1.120 lr:1.6e-05 updt_s:4.696 data_s:0.000\n",
      "WARNING 2025-08-11 14:55:47 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:55:47 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:55:52 in_model.py:238 step:158 smpl:55K ep:131 epch:2.38 loss:1.000 grdn:1.284 lr:1.6e-05 updt_s:4.730 data_s:0.000\n",
      "WARNING 2025-08-11 14:55:52 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:55:52 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:55:57 in_model.py:238 step:159 smpl:56K ep:132 epch:2.40 loss:0.981 grdn:1.248 lr:1.6e-05 updt_s:4.703 data_s:0.000\n",
      "WARNING 2025-08-11 14:55:57 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:55:57 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:56:01 in_model.py:238 step:160 smpl:56K ep:133 epch:2.41 loss:0.971 grdn:1.197 lr:1.6e-05 updt_s:4.700 data_s:0.000\n",
      "WARNING 2025-08-11 14:56:01 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:56:01 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 160 of 1000\n",
      "INFO 2025-08-11 14:56:06 in_model.py:238 step:161 smpl:56K ep:134 epch:2.43 loss:0.990 grdn:1.194 lr:1.6e-05 updt_s:4.703 data_s:0.000\n",
      "WARNING 2025-08-11 14:56:06 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:56:06 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:56:11 in_model.py:238 step:162 smpl:57K ep:134 epch:2.45 loss:0.972 grdn:1.160 lr:1.6e-05 updt_s:4.727 data_s:0.000\n",
      "WARNING 2025-08-11 14:56:11 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:56:11 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:56:16 in_model.py:238 step:163 smpl:57K ep:135 epch:2.46 loss:0.969 grdn:1.195 lr:1.6e-05 updt_s:4.715 data_s:0.000\n",
      "WARNING 2025-08-11 14:56:16 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:56:16 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:56:20 in_model.py:238 step:164 smpl:57K ep:136 epch:2.48 loss:0.980 grdn:1.218 lr:1.6e-05 updt_s:4.697 data_s:0.000\n",
      "WARNING 2025-08-11 14:56:20 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:56:20 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:56:25 in_model.py:238 step:165 smpl:58K ep:137 epch:2.49 loss:0.968 grdn:1.268 lr:1.7e-05 updt_s:4.781 data_s:0.000\n",
      "WARNING 2025-08-11 14:56:25 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:56:25 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:56:30 in_model.py:238 step:166 smpl:58K ep:138 epch:2.51 loss:0.959 grdn:1.177 lr:1.7e-05 updt_s:4.714 data_s:0.000\n",
      "WARNING 2025-08-11 14:56:30 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:56:30 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:56:35 in_model.py:238 step:167 smpl:58K ep:139 epch:2.52 loss:0.934 grdn:1.171 lr:1.7e-05 updt_s:4.712 data_s:0.000\n",
      "WARNING 2025-08-11 14:56:35 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:56:35 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:56:39 in_model.py:238 step:168 smpl:59K ep:139 epch:2.54 loss:0.918 grdn:1.283 lr:1.7e-05 updt_s:4.717 data_s:0.000\n",
      "WARNING 2025-08-11 14:56:39 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:56:39 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:56:44 in_model.py:238 step:169 smpl:59K ep:140 epch:2.55 loss:0.951 grdn:1.179 lr:1.7e-05 updt_s:5.011 data_s:0.000\n",
      "WARNING 2025-08-11 14:56:44 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:56:44 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:56:49 in_model.py:238 step:170 smpl:60K ep:141 epch:2.57 loss:0.952 grdn:1.355 lr:1.7e-05 updt_s:4.698 data_s:0.000\n",
      "WARNING 2025-08-11 14:56:49 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:56:49 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 170 of 1000\n",
      "INFO 2025-08-11 14:56:54 in_model.py:238 step:171 smpl:60K ep:142 epch:2.58 loss:0.913 grdn:1.146 lr:1.7e-05 updt_s:4.700 data_s:0.000\n",
      "WARNING 2025-08-11 14:56:54 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:56:54 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:56:58 in_model.py:238 step:172 smpl:60K ep:143 epch:2.60 loss:0.925 grdn:1.183 lr:1.7e-05 updt_s:4.705 data_s:0.000\n",
      "WARNING 2025-08-11 14:56:58 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:56:58 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:57:03 in_model.py:238 step:173 smpl:61K ep:144 epch:2.61 loss:0.914 grdn:1.373 lr:1.7e-05 updt_s:4.697 data_s:0.000\n",
      "WARNING 2025-08-11 14:57:03 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:57:03 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:57:08 in_model.py:238 step:174 smpl:61K ep:144 epch:2.63 loss:0.890 grdn:1.237 lr:1.7e-05 updt_s:4.708 data_s:0.000\n",
      "WARNING 2025-08-11 14:57:08 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:57:08 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:57:13 in_model.py:238 step:175 smpl:61K ep:145 epch:2.64 loss:0.902 grdn:1.284 lr:1.8e-05 updt_s:4.710 data_s:0.000\n",
      "WARNING 2025-08-11 14:57:13 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:57:13 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:57:17 in_model.py:238 step:176 smpl:62K ep:146 epch:2.66 loss:0.883 grdn:1.224 lr:1.8e-05 updt_s:4.710 data_s:0.000\n",
      "WARNING 2025-08-11 14:57:17 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:57:17 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:57:22 in_model.py:238 step:177 smpl:62K ep:147 epch:2.67 loss:0.914 grdn:1.365 lr:1.8e-05 updt_s:4.697 data_s:0.000\n",
      "WARNING 2025-08-11 14:57:22 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:57:22 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:57:27 in_model.py:238 step:178 smpl:62K ep:148 epch:2.69 loss:0.891 grdn:1.525 lr:1.8e-05 updt_s:4.709 data_s:0.000\n",
      "WARNING 2025-08-11 14:57:27 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:57:27 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:57:31 in_model.py:238 step:179 smpl:63K ep:149 epch:2.70 loss:0.854 grdn:1.333 lr:1.8e-05 updt_s:4.709 data_s:0.000\n",
      "WARNING 2025-08-11 14:57:31 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:57:31 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:57:36 in_model.py:238 step:180 smpl:63K ep:149 epch:2.72 loss:0.902 grdn:1.446 lr:1.8e-05 updt_s:4.709 data_s:0.000\n",
      "WARNING 2025-08-11 14:57:36 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:57:36 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 180 of 1000\n",
      "INFO 2025-08-11 14:57:41 in_model.py:238 step:181 smpl:63K ep:150 epch:2.73 loss:0.837 grdn:1.480 lr:1.8e-05 updt_s:4.701 data_s:0.000\n",
      "WARNING 2025-08-11 14:57:41 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:57:41 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:57:46 in_model.py:238 step:182 smpl:64K ep:151 epch:2.75 loss:0.871 grdn:1.451 lr:1.8e-05 updt_s:4.714 data_s:0.000\n",
      "WARNING 2025-08-11 14:57:46 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:57:46 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:57:50 in_model.py:238 step:183 smpl:64K ep:152 epch:2.76 loss:0.840 grdn:1.361 lr:1.8e-05 updt_s:4.708 data_s:0.000\n",
      "WARNING 2025-08-11 14:57:50 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:57:50 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:57:55 in_model.py:238 step:184 smpl:64K ep:153 epch:2.78 loss:0.837 grdn:1.411 lr:1.8e-05 updt_s:4.708 data_s:0.000\n",
      "WARNING 2025-08-11 14:57:55 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:57:55 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:58:00 in_model.py:238 step:185 smpl:65K ep:154 epch:2.79 loss:0.794 grdn:1.485 lr:1.9e-05 updt_s:4.714 data_s:0.000\n",
      "WARNING 2025-08-11 14:58:00 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:58:00 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:58:04 in_model.py:238 step:186 smpl:65K ep:154 epch:2.81 loss:0.798 grdn:1.635 lr:1.9e-05 updt_s:4.702 data_s:0.000\n",
      "WARNING 2025-08-11 14:58:04 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:58:04 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:58:09 in_model.py:238 step:187 smpl:65K ep:155 epch:2.82 loss:0.789 grdn:1.522 lr:1.9e-05 updt_s:4.707 data_s:0.000\n",
      "WARNING 2025-08-11 14:58:09 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:58:09 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:58:14 in_model.py:238 step:188 smpl:66K ep:156 epch:2.84 loss:0.798 grdn:1.741 lr:1.9e-05 updt_s:4.712 data_s:0.000\n",
      "WARNING 2025-08-11 14:58:14 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:58:14 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:58:19 in_model.py:238 step:189 smpl:66K ep:157 epch:2.85 loss:0.781 grdn:1.488 lr:1.9e-05 updt_s:4.704 data_s:0.000\n",
      "WARNING 2025-08-11 14:58:19 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:58:19 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:58:23 in_model.py:238 step:190 smpl:66K ep:158 epch:2.87 loss:0.825 grdn:1.604 lr:1.9e-05 updt_s:4.717 data_s:0.000\n",
      "WARNING 2025-08-11 14:58:23 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:58:23 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 190 of 1000\n",
      "INFO 2025-08-11 14:58:28 in_model.py:238 step:191 smpl:67K ep:159 epch:2.88 loss:0.746 grdn:1.429 lr:1.9e-05 updt_s:4.710 data_s:0.000\n",
      "WARNING 2025-08-11 14:58:28 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:58:28 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:58:33 in_model.py:238 step:192 smpl:67K ep:159 epch:2.90 loss:0.812 grdn:1.682 lr:1.9e-05 updt_s:4.713 data_s:0.000\n",
      "WARNING 2025-08-11 14:58:33 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:58:33 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:58:37 in_model.py:238 step:193 smpl:68K ep:160 epch:2.91 loss:0.764 grdn:1.790 lr:1.9e-05 updt_s:4.712 data_s:0.000\n",
      "WARNING 2025-08-11 14:58:37 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:58:37 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:58:42 in_model.py:238 step:194 smpl:68K ep:161 epch:2.93 loss:0.740 grdn:1.411 lr:1.9e-05 updt_s:4.705 data_s:0.000\n",
      "WARNING 2025-08-11 14:58:42 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:58:42 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:58:47 in_model.py:238 step:195 smpl:68K ep:162 epch:2.94 loss:0.732 grdn:1.503 lr:2.0e-05 updt_s:4.701 data_s:0.000\n",
      "WARNING 2025-08-11 14:58:47 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:58:47 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:58:52 in_model.py:238 step:196 smpl:69K ep:163 epch:2.96 loss:0.713 grdn:1.692 lr:2.0e-05 updt_s:4.697 data_s:0.000\n",
      "WARNING 2025-08-11 14:58:52 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:58:52 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:58:56 in_model.py:238 step:197 smpl:69K ep:164 epch:2.97 loss:0.736 grdn:1.670 lr:2.0e-05 updt_s:4.702 data_s:0.000\n",
      "WARNING 2025-08-11 14:58:56 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:58:56 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:59:01 in_model.py:238 step:198 smpl:69K ep:164 epch:2.99 loss:0.738 grdn:1.527 lr:2.0e-05 updt_s:4.704 data_s:0.000\n",
      "WARNING 2025-08-11 14:59:01 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:59:01 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:59:06 in_model.py:238 step:199 smpl:70K ep:165 epch:3.00 loss:0.726 grdn:1.515 lr:2.0e-05 updt_s:4.697 data_s:0.000\n",
      "WARNING 2025-08-11 14:59:06 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:59:06 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:59:10 in_model.py:238 step:200 smpl:70K ep:166 epch:3.02 loss:0.699 grdn:1.561 lr:2.0e-05 updt_s:4.700 data_s:0.000\n",
      "WARNING 2025-08-11 14:59:10 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:59:10 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:59:10 in_model.py:247 Checkpoint policy after step 200\n",
      "Step 200 of 1000\n",
      "INFO 2025-08-11 14:59:16 in_model.py:238 step:201 smpl:70K ep:167 epch:3.03 loss:0.755 grdn:2.477 lr:2.0e-05 updt_s:1.251 data_s:0.001\n",
      "WARNING 2025-08-11 14:59:16 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:59:16 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-08-11 14:59:35 in_model.py:238 step:202 smpl:71K ep:168 epch:3.05 loss:0.734 grdn:2.045 lr:2.0e-05 updt_s:4.827 data_s:14.232\n",
      "WARNING 2025-08-11 14:59:35 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:59:35 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:59:40 in_model.py:238 step:203 smpl:71K ep:169 epch:3.06 loss:0.703 grdn:1.900 lr:2.0e-05 updt_s:4.717 data_s:0.001\n",
      "WARNING 2025-08-11 14:59:40 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:59:40 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:59:44 in_model.py:238 step:204 smpl:71K ep:169 epch:3.08 loss:0.727 grdn:1.803 lr:2.0e-05 updt_s:4.708 data_s:0.001\n",
      "WARNING 2025-08-11 14:59:44 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:59:44 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:59:49 in_model.py:238 step:205 smpl:72K ep:170 epch:3.09 loss:0.675 grdn:1.614 lr:2.1e-05 updt_s:4.691 data_s:0.001\n",
      "WARNING 2025-08-11 14:59:49 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:59:49 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:59:54 in_model.py:238 step:206 smpl:72K ep:171 epch:3.11 loss:0.689 grdn:1.462 lr:2.1e-05 updt_s:4.801 data_s:0.000\n",
      "WARNING 2025-08-11 14:59:54 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:59:54 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 14:59:59 in_model.py:238 step:207 smpl:72K ep:172 epch:3.12 loss:0.650 grdn:1.701 lr:2.1e-05 updt_s:4.739 data_s:0.001\n",
      "WARNING 2025-08-11 14:59:59 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 14:59:59 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 15:00:03 in_model.py:238 step:208 smpl:73K ep:173 epch:3.14 loss:0.656 grdn:1.747 lr:2.1e-05 updt_s:4.707 data_s:0.000\n",
      "WARNING 2025-08-11 15:00:03 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 15:00:03 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 15:00:08 in_model.py:238 step:209 smpl:73K ep:173 epch:3.15 loss:0.700 grdn:1.683 lr:2.1e-05 updt_s:4.701 data_s:0.001\n",
      "WARNING 2025-08-11 15:00:08 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 15:00:08 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-08-11 15:00:13 in_model.py:238 step:210 smpl:74K ep:174 epch:3.17 loss:0.641 grdn:1.647 lr:2.1e-05 updt_s:4.705 data_s:0.001\n",
      "WARNING 2025-08-11 15:00:13 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 15:00:13 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "Step 210 of 1000\n",
      "INFO 2025-08-11 15:00:18 in_model.py:238 step:211 smpl:74K ep:175 epch:3.18 loss:0.667 grdn:1.685 lr:2.1e-05 updt_s:4.698 data_s:0.001\n",
      "WARNING 2025-08-11 15:00:18 db_utils.py:117 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-08-11 15:00:18 db_utils.py:117 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ckpt\n",
    "!python3 train_model.py --config_path smolvla_omy.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574793c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli download Lava8888/smolvla-omy-checkpoints  --local-dir ./smolvla_omy_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63359af",
   "metadata": {},
   "source": [
    "# Uppload checkpoint 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63408123",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli upload smolvla_omy_ckpt_000100 /home/ludwig/Qualia/hyperstack_smolvla/smolvla_omy_checkpoints/smolvla_omy/checkpoints/000100/pretrained_model/config.json --repo-type=model\n",
    "!huggingface-cli upload smolvla_omy_ckpt_000100 /home/ludwig/Qualia/hyperstack_smolvla/smolvla_omy_checkpoints/smolvla_omy/checkpoints/000100/pretrained_model/model.safetensors --repo-type=model\n",
    "!huggingface-cli upload smolvla_omy_ckpt_000100 /home/ludwig/Qualia/hyperstack_smolvla/smolvla_omy_checkpoints/smolvla_omy/checkpoints/000100/pretrained_model/train_config.json --repo-type=model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a7dc2b",
   "metadata": {},
   "source": [
    "# Uppload checkpoint 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea45cfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli upload smolvla_omy_ckpt_000200 /home/ludwig/Qualia/hyperstack_smolvla/smolvla_omy_checkpoints/smolvla_omy/checkpoints/000200/pretrained_model/config.json --repo-type=model\n",
    "!huggingface-cli upload smolvla_omy_ckpt_000200 /home/ludwig/Qualia/hyperstack_smolvla/smolvla_omy_checkpoints/smolvla_omy/checkpoints/000200/pretrained_model/model.safetensors --repo-type=model\n",
    "!huggingface-cli upload smolvla_omy_ckpt_000200 /home/ludwig/Qualia/hyperstack_smolvla/smolvla_omy_checkpoints/smolvla_omy/checkpoints/000200/pretrained_model/train_config.json --repo-type=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "960900a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli upload' is deprecated. Use 'hf upload' instead.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 61, in main\n",
      "    service.run()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/commands/upload.py\", line 211, in run\n",
      "    print(self._upload())\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/commands/upload.py\", line 271, in _upload\n",
      "    raise FileNotFoundError(f\"No such file or directory: '{self.local_path}'.\")\n",
      "FileNotFoundError: No such file or directory: '/home/ludwig/Qualia/hyperstack_smolvla/smolvla_omy_checkpoints/smolvla_omy/checkpoints/000750/pretrained_model/config.json'.\n",
      "\u001b[33m⚠️  Warning: 'huggingface-cli upload' is deprecated. Use 'hf upload' instead.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 61, in main\n",
      "    service.run()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/commands/upload.py\", line 211, in run\n",
      "    print(self._upload())\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/commands/upload.py\", line 271, in _upload\n",
      "    raise FileNotFoundError(f\"No such file or directory: '{self.local_path}'.\")\n",
      "FileNotFoundError: No such file or directory: '/home/ludwig/Qualia/hyperstack_smolvla/smolvla_omy_checkpoints/smolvla_omy/checkpoints/000750/pretrained_model/model.safetensors'.\n",
      "\u001b[33m⚠️  Warning: 'huggingface-cli upload' is deprecated. Use 'hf upload' instead.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 61, in main\n",
      "    service.run()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/commands/upload.py\", line 211, in run\n",
      "    print(self._upload())\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/commands/upload.py\", line 271, in _upload\n",
      "    raise FileNotFoundError(f\"No such file or directory: '{self.local_path}'.\")\n",
      "FileNotFoundError: No such file or directory: '/home/ludwig/Qualia/hyperstack_smolvla/smolvla_omy_checkpoints/smolvla_omy/checkpoints/000750/pretrained_model/train_config.json'.\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli upload smolvla_omy_ckpt_000750 /home/ludwig/Qualia/hyperstack_smolvla/smolvla_omy_checkpoints/smolvla_omy/checkpoints/000750/pretrained_model/config.json --repo-type=model\n",
    "!huggingface-cli upload smolvla_omy_ckpt_000750 /home/ludwig/Qualia/hyperstack_smolvla/smolvla_omy_checkpoints/smolvla_omy/checkpoints/000750/pretrained_model/model.safetensors --repo-type=model\n",
    "!huggingface-cli upload smolvla_omy_ckpt_000750 /home/ludwig/Qualia/hyperstack_smolvla/smolvla_omy_checkpoints/smolvla_omy/checkpoints/000750/pretrained_model/train_config.json --repo-type=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a5781c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/last/pretrained_model/config.json to lava8888/CounterToSink/pretrained_model/config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/last/pretrained_model/train_config.json to lava8888/CounterToSink/pretrained_model/train_config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/last/pretrained_model/model.safetensors to lava8888/CounterToSink/pretrained_model/model.safetensors\n",
      "Uploading ckpt/smolvla_omy/checkpoints/last/training_state/optimizer_param_groups.json to lava8888/CounterToSink/training_state/optimizer_param_groups.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/last/training_state/optimizer_state.safetensors to lava8888/CounterToSink/training_state/optimizer_state.safetensors\n",
      "Uploading ckpt/smolvla_omy/checkpoints/last/training_state/training_step.json to lava8888/CounterToSink/training_state/training_step.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/last/training_state/rng_state.safetensors to lava8888/CounterToSink/training_state/rng_state.safetensors\n",
      "Uploading ckpt/smolvla_omy/checkpoints/last/training_state/scheduler_state.json to lava8888/CounterToSink/training_state/scheduler_state.json\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "api = HfApi()\n",
    "repo_id = \"lava8888/CounterToSink\"  # Correct format\n",
    "\n",
    "local_path = \"ckpt/smolvla_omy/checkpoints/last\"\n",
    "for root, dirs, files in os.walk(local_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        remote_path = os.path.relpath(file_path, local_path)\n",
    "        print(f\"Uploading {file_path} to {repo_id}/{remote_path}\")\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=file_path,\n",
    "            path_in_repo=f\"last/{remote_path}\",\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"model\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10845e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000750/pretrained_model/config.json to Lava8888/CounterToSink/000750/pretrained_model/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000750/pretrained_model/train_config.json to Lava8888/CounterToSink/000750/pretrained_model/train_config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000750/pretrained_model/model.safetensors to Lava8888/CounterToSink/000750/pretrained_model/model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 1.20G/1.20G [00:22<00:00, 53.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000750/training_state/optimizer_param_groups.json to Lava8888/CounterToSink/000750/training_state/optimizer_param_groups.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000750/training_state/optimizer_state.safetensors to Lava8888/CounterToSink/000750/training_state/optimizer_state.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "optimizer_state.safetensors: 100%|██████████| 413M/413M [00:08<00:00, 50.9MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000750/training_state/training_step.json to Lava8888/CounterToSink/000750/training_state/training_step.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000750/training_state/rng_state.safetensors to Lava8888/CounterToSink/000750/training_state/rng_state.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rng_state.safetensors: 100%|██████████| 15.7k/15.7k [00:00<00:00, 210kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000750/training_state/scheduler_state.json to Lava8888/CounterToSink/000750/training_state/scheduler_state.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000250/pretrained_model/config.json to Lava8888/CounterToSink/000250/pretrained_model/config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000250/pretrained_model/train_config.json to Lava8888/CounterToSink/000250/pretrained_model/train_config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000250/pretrained_model/model.safetensors to Lava8888/CounterToSink/000250/pretrained_model/model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 1.20G/1.20G [00:23<00:00, 50.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000250/training_state/optimizer_param_groups.json to Lava8888/CounterToSink/000250/training_state/optimizer_param_groups.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000250/training_state/optimizer_state.safetensors to Lava8888/CounterToSink/000250/training_state/optimizer_state.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "optimizer_state.safetensors: 100%|██████████| 413M/413M [00:07<00:00, 52.9MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000250/training_state/training_step.json to Lava8888/CounterToSink/000250/training_state/training_step.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000250/training_state/rng_state.safetensors to Lava8888/CounterToSink/000250/training_state/rng_state.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rng_state.safetensors: 100%|██████████| 15.7k/15.7k [00:00<00:00, 259kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000250/training_state/scheduler_state.json to Lava8888/CounterToSink/000250/training_state/scheduler_state.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000200/pretrained_model/config.json to Lava8888/CounterToSink/000200/pretrained_model/config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000200/pretrained_model/train_config.json to Lava8888/CounterToSink/000200/pretrained_model/train_config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000200/pretrained_model/model.safetensors to Lava8888/CounterToSink/000200/pretrained_model/model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 1.20G/1.20G [00:24<00:00, 48.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000200/training_state/optimizer_param_groups.json to Lava8888/CounterToSink/000200/training_state/optimizer_param_groups.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000200/training_state/optimizer_state.safetensors to Lava8888/CounterToSink/000200/training_state/optimizer_state.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "optimizer_state.safetensors: 100%|██████████| 413M/413M [00:07<00:00, 52.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000200/training_state/training_step.json to Lava8888/CounterToSink/000200/training_state/training_step.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000200/training_state/rng_state.safetensors to Lava8888/CounterToSink/000200/training_state/rng_state.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rng_state.safetensors: 100%|██████████| 15.7k/15.7k [00:00<00:00, 182kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000200/training_state/scheduler_state.json to Lava8888/CounterToSink/000200/training_state/scheduler_state.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000700/pretrained_model/config.json to Lava8888/CounterToSink/000700/pretrained_model/config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000700/pretrained_model/train_config.json to Lava8888/CounterToSink/000700/pretrained_model/train_config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000700/pretrained_model/model.safetensors to Lava8888/CounterToSink/000700/pretrained_model/model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 1.20G/1.20G [00:21<00:00, 55.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000700/training_state/optimizer_param_groups.json to Lava8888/CounterToSink/000700/training_state/optimizer_param_groups.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000700/training_state/optimizer_state.safetensors to Lava8888/CounterToSink/000700/training_state/optimizer_state.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "optimizer_state.safetensors: 100%|██████████| 413M/413M [00:09<00:00, 41.5MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000700/training_state/training_step.json to Lava8888/CounterToSink/000700/training_state/training_step.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000700/training_state/rng_state.safetensors to Lava8888/CounterToSink/000700/training_state/rng_state.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rng_state.safetensors: 100%|██████████| 15.7k/15.7k [00:00<00:00, 342kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000700/training_state/scheduler_state.json to Lava8888/CounterToSink/000700/training_state/scheduler_state.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000100/pretrained_model/config.json to Lava8888/CounterToSink/000100/pretrained_model/config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000100/pretrained_model/train_config.json to Lava8888/CounterToSink/000100/pretrained_model/train_config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000100/pretrained_model/model.safetensors to Lava8888/CounterToSink/000100/pretrained_model/model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 1.20G/1.20G [00:20<00:00, 57.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000100/training_state/optimizer_param_groups.json to Lava8888/CounterToSink/000100/training_state/optimizer_param_groups.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000100/training_state/optimizer_state.safetensors to Lava8888/CounterToSink/000100/training_state/optimizer_state.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "optimizer_state.safetensors: 100%|██████████| 413M/413M [00:07<00:00, 55.6MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000100/training_state/training_step.json to Lava8888/CounterToSink/000100/training_state/training_step.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000100/training_state/rng_state.safetensors to Lava8888/CounterToSink/000100/training_state/rng_state.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rng_state.safetensors: 100%|██████████| 15.7k/15.7k [00:00<00:00, 173kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000100/training_state/scheduler_state.json to Lava8888/CounterToSink/000100/training_state/scheduler_state.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000650/pretrained_model/config.json to Lava8888/CounterToSink/000650/pretrained_model/config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000650/pretrained_model/train_config.json to Lava8888/CounterToSink/000650/pretrained_model/train_config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000650/pretrained_model/model.safetensors to Lava8888/CounterToSink/000650/pretrained_model/model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 1.20G/1.20G [00:23<00:00, 51.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000650/training_state/optimizer_param_groups.json to Lava8888/CounterToSink/000650/training_state/optimizer_param_groups.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000650/training_state/optimizer_state.safetensors to Lava8888/CounterToSink/000650/training_state/optimizer_state.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "optimizer_state.safetensors: 100%|██████████| 413M/413M [00:06<00:00, 59.5MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000650/training_state/training_step.json to Lava8888/CounterToSink/000650/training_state/training_step.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000650/training_state/rng_state.safetensors to Lava8888/CounterToSink/000650/training_state/rng_state.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rng_state.safetensors: 100%|██████████| 15.7k/15.7k [00:00<00:00, 198kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000650/training_state/scheduler_state.json to Lava8888/CounterToSink/000650/training_state/scheduler_state.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000300/pretrained_model/config.json to Lava8888/CounterToSink/000300/pretrained_model/config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000300/pretrained_model/train_config.json to Lava8888/CounterToSink/000300/pretrained_model/train_config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000300/pretrained_model/model.safetensors to Lava8888/CounterToSink/000300/pretrained_model/model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 1.20G/1.20G [00:21<00:00, 54.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000300/training_state/optimizer_param_groups.json to Lava8888/CounterToSink/000300/training_state/optimizer_param_groups.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000300/training_state/optimizer_state.safetensors to Lava8888/CounterToSink/000300/training_state/optimizer_state.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "optimizer_state.safetensors: 100%|██████████| 413M/413M [00:08<00:00, 50.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000300/training_state/training_step.json to Lava8888/CounterToSink/000300/training_state/training_step.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000300/training_state/rng_state.safetensors to Lava8888/CounterToSink/000300/training_state/rng_state.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rng_state.safetensors: 100%|██████████| 15.7k/15.7k [00:00<00:00, 182kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/000300/training_state/scheduler_state.json to Lava8888/CounterToSink/000300/training_state/scheduler_state.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000050/pretrained_model/config.json to Lava8888/CounterToSink/000050/pretrained_model/config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000050/pretrained_model/train_config.json to Lava8888/CounterToSink/000050/pretrained_model/train_config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/000050/pretrained_model/model.safetensors to Lava8888/CounterToSink/000050/pretrained_model/model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:   5%|▌         | 64.0M/1.20G [00:01<00:20, 54.3MB/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m remote_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrelpath(local_path, local_dir)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremote_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1669\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1666\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4710\u001b[0m, in \u001b[0;36mHfApi.upload_file\u001b[0;34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   4702\u001b[0m commit_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4703\u001b[0m     commit_message \u001b[38;5;28;01mif\u001b[39;00m commit_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with huggingface_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4704\u001b[0m )\n\u001b[1;32m   4705\u001b[0m operation \u001b[38;5;241m=\u001b[39m CommitOperationAdd(\n\u001b[1;32m   4706\u001b[0m     path_or_fileobj\u001b[38;5;241m=\u001b[39mpath_or_fileobj,\n\u001b[1;32m   4707\u001b[0m     path_in_repo\u001b[38;5;241m=\u001b[39mpath_in_repo,\n\u001b[1;32m   4708\u001b[0m )\n\u001b[0;32m-> 4710\u001b[0m commit_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4713\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4720\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_info\u001b[38;5;241m.\u001b[39mpr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4723\u001b[0m     revision \u001b[38;5;241m=\u001b[39m quote(_parse_revision_from_pr_url(commit_info\u001b[38;5;241m.\u001b[39mpr_url), safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1669\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1666\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4239\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   4236\u001b[0m \u001b[38;5;66;03m# If updating twice the same file or update then delete a file in a single commit\u001b[39;00m\n\u001b[1;32m   4237\u001b[0m _warn_on_overwriting_operations(operations)\n\u001b[0;32m-> 4239\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreupload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4241\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munquoted_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# first-class methods take unquoted revision\u001b[39;49;00m\n\u001b[1;32m   4245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfree_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# do not remove `CommitOperationAdd.path_or_fileobj` on LFS files for \"normal\" users\u001b[39;49;00m\n\u001b[1;32m   4248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4250\u001b[0m files_to_copy \u001b[38;5;241m=\u001b[39m _fetch_files_to_copy(\n\u001b[1;32m   4251\u001b[0m     copies\u001b[38;5;241m=\u001b[39mcopies,\n\u001b[1;32m   4252\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4256\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint,\n\u001b[1;32m   4257\u001b[0m )\n\u001b[1;32m   4258\u001b[0m \u001b[38;5;66;03m# Remove no-op operations (files that have not changed)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4531\u001b[0m, in \u001b[0;36mHfApi.preupload_lfs_files\u001b[0;34m(self, repo_id, additions, token, repo_type, revision, create_pr, num_threads, free_memory, gitignore_content)\u001b[0m\n\u001b[1;32m   4526\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m has_buffered_io_data:\n\u001b[1;32m   4527\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   4528\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading files as a binary IO buffer is not supported by Xet Storage. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4529\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to HTTP upload.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4530\u001b[0m             )\n\u001b[0;32m-> 4531\u001b[0m     \u001b[43m_upload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mupload_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore [arg-type]\u001b[39;00m\n\u001b[1;32m   4532\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m addition \u001b[38;5;129;01min\u001b[39;00m new_lfs_additions_to_upload:\n\u001b[1;32m   4533\u001b[0m     addition\u001b[38;5;241m.\u001b[39m_is_uploaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/_commit_api.py:452\u001b[0m, in \u001b[0;36m_upload_lfs_files\u001b[0;34m(additions, repo_type, repo_id, headers, endpoint, num_threads, revision)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filtered_actions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    451\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading 1 LFS file to the Hub\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 452\u001b[0m     \u001b[43m_wrapped_lfs_upload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_actions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_actions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m LFS files to the Hub using up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_threads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m threads concurrently\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/_commit_api.py:442\u001b[0m, in \u001b[0;36m_upload_lfs_files.<locals>._wrapped_lfs_upload\u001b[0;34m(batch_action)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     operation \u001b[38;5;241m=\u001b[39m oid2addop[batch_action[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moid\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m--> 442\u001b[0m     \u001b[43mlfs_upload\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlfs_batch_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while uploading \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation\u001b[38;5;241m.\u001b[39mpath_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/lfs.py:246\u001b[0m, in \u001b[0;36mlfs_upload\u001b[0;34m(operation, lfs_batch_action, token, headers, endpoint)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    244\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMalformed response from LFS batch endpoint: `chunk_size` should be an integer. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    245\u001b[0m         )\n\u001b[0;32m--> 246\u001b[0m     \u001b[43m_upload_multi_part\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupload_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupload_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     _upload_single_part(operation\u001b[38;5;241m=\u001b[39moperation, upload_url\u001b[38;5;241m=\u001b[39mupload_url)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/lfs.py:346\u001b[0m, in \u001b[0;36m_upload_multi_part\u001b[0;34m(operation, header, chunk_size, upload_url)\u001b[0m\n\u001b[1;32m    337\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_transfer is enabled but does not support uploading from bytes or BinaryIO, falling back to regular\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m upload\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    340\u001b[0m     )\n\u001b[1;32m    341\u001b[0m     use_hf_transfer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    343\u001b[0m response_headers \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    344\u001b[0m     _upload_parts_hf_transfer(operation\u001b[38;5;241m=\u001b[39moperation, sorted_parts_urls\u001b[38;5;241m=\u001b[39msorted_parts_urls, chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_hf_transfer\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_upload_parts_iteratively\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_parts_urls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorted_parts_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# 3. Send completion request\u001b[39;00m\n\u001b[1;32m    350\u001b[0m completion_res \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    351\u001b[0m     upload_url,\n\u001b[1;32m    352\u001b[0m     json\u001b[38;5;241m=\u001b[39m_get_completion_payload(response_headers, operation\u001b[38;5;241m.\u001b[39mupload_info\u001b[38;5;241m.\u001b[39msha256\u001b[38;5;241m.\u001b[39mhex()),\n\u001b[1;32m    353\u001b[0m     headers\u001b[38;5;241m=\u001b[39mLFS_HEADERS,\n\u001b[1;32m    354\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/lfs.py:403\u001b[0m, in \u001b[0;36m_upload_parts_iteratively\u001b[0;34m(operation, sorted_parts_urls, chunk_size)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part_idx, part_upload_url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sorted_parts_urls):\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m SliceFileObj(\n\u001b[1;32m    398\u001b[0m         fileobj,\n\u001b[1;32m    399\u001b[0m         seek_from\u001b[38;5;241m=\u001b[39mchunk_size \u001b[38;5;241m*\u001b[39m part_idx,\n\u001b[1;32m    400\u001b[0m         read_limit\u001b[38;5;241m=\u001b[39mchunk_size,\n\u001b[1;32m    401\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m fileobj_slice:\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;66;03m# S3 might raise a transient 500 error -> let's retry if that happens\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m         part_upload_res \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPUT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_upload_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfileobj_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m502\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m503\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m504\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m         hf_raise_for_status(part_upload_res)\n\u001b[1;32m    407\u001b[0m         headers\u001b[38;5;241m.\u001b[39mappend(part_upload_res\u001b[38;5;241m.\u001b[39mheaders)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:310\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:96\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:493\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:508\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%x\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(chunk), chunk))\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Regardless of whether we have a body or not, if we're in\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# chunked mode we want to send an explicit empty chunk.\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunked:\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:999\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    997\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.send\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, data)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 999\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mIterable):\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1266\u001b[0m, in \u001b[0;36mSSLSocket.sendall\u001b[0;34m(self, data, flags)\u001b[0m\n\u001b[1;32m   1264\u001b[0m         amount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(byte_view)\n\u001b[1;32m   1265\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m count \u001b[38;5;241m<\u001b[39m amount:\n\u001b[0;32m-> 1266\u001b[0m             v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1267\u001b[0m             count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1235\u001b[0m, in \u001b[0;36mSSLSocket.send\u001b[0;34m(self, data, flags)\u001b[0m\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1232\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1233\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to send() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1234\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msend(data, flags)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "api = HfApi()\n",
    "repo_id = \"Lava8888/CounterToSink\"  # Replace with your username\n",
    "\n",
    "local_dir = \"ckpt/smolvla_omy/checkpoints\"\n",
    "for root, dirs, files in os.walk(local_dir):\n",
    "    for file in files:\n",
    "        local_path = os.path.join(root, file)\n",
    "        remote_path = os.path.relpath(local_path, local_dir)\n",
    "        print(f\"Uploading {local_path} to {repo_id}/{remote_path}\")\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=local_path,\n",
    "            path_in_repo=remote_path,\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"model\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b62b5",
   "metadata": {},
   "source": [
    "# separate repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50957f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface-cli repo create CounterToSinkLast --type=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "935a1b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/last/pretrained_model/config.json to lava8888/CounterToSinkLast/pretrained_model/config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/last/pretrained_model/train_config.json to lava8888/CounterToSinkLast/pretrained_model/train_config.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/last/pretrained_model/model.safetensors to lava8888/CounterToSinkLast/pretrained_model/model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 1.20G/1.20G [00:21<00:00, 54.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/last/training_state/optimizer_param_groups.json to lava8888/CounterToSinkLast/training_state/optimizer_param_groups.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/last/training_state/optimizer_state.safetensors to lava8888/CounterToSinkLast/training_state/optimizer_state.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "optimizer_state.safetensors: 100%|██████████| 413M/413M [00:06<00:00, 59.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/last/training_state/training_step.json to lava8888/CounterToSinkLast/training_state/training_step.json\n",
      "Uploading ckpt/smolvla_omy/checkpoints/last/training_state/rng_state.safetensors to lava8888/CounterToSinkLast/training_state/rng_state.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rng_state.safetensors: 100%|██████████| 15.7k/15.7k [00:00<00:00, 231kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ckpt/smolvla_omy/checkpoints/last/training_state/scheduler_state.json to lava8888/CounterToSinkLast/training_state/scheduler_state.json\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "api = HfApi()\n",
    "repo_id = \"lava8888/CounterToSinkLast\"\n",
    "\n",
    "local_path = \"ckpt/smolvla_omy/checkpoints/last\"\n",
    "for root, dirs, files in os.walk(local_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        remote_path = os.path.relpath(file_path, local_path)\n",
    "        print(f\"Uploading {file_path} to {repo_id}/{remote_path}\")\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=file_path,\n",
    "            path_in_repo=remote_path,\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"model\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b908628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
